[{"id":"beltagySciBERTPretrainedLanguage2019","abstract":"Obtaining large-scale annotated data for NLP tasks in the scientiﬁc domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of highquality, large-scale labeled scientiﬁc data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientiﬁc publications to improve performance on downstream scientiﬁc NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classiﬁcation and dependency parsing, with datasets from a variety of scientiﬁc domains. We demonstrate statistically signiﬁcant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github. com/allenai/scibert/.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Beltagy","given":"Iz"},{"family":"Lo","given":"Kyle"},{"family":"Cohan","given":"Arman"}],"citation-key":"beltagySciBERTPretrainedLanguage2019","container-title":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","DOI":"10.18653/v1/D19-1371","event-place":"Hong Kong, China","event-title":"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)","issued":{"date-parts":[["2019"]]},"language":"en","page":"3613-3618","publisher":"Association for Computational Linguistics","publisher-place":"Hong Kong, China","source":"DOI.org (Crossref)","title":"SciBERT: A Pretrained Language Model for Scientific Text","title-short":"SciBERT","type":"paper-conference","URL":"https://www.aclweb.org/anthology/D19-1371"},{"id":"berkmanUsefulGoodTheory","abstract":"Practicality was a valued attribute of academic psychological theory during its initial decades, but usefulness has since faded in importance to the field. Theories are now evaluated mainly on their ability to account for decontextualized laboratory data and not their ability to help solve societal problems. With laudable exceptions in the clinical, intergroup, and health domains, most psychological theories have little relevance to people’s everyday lives, poor accessibility to policymakers, or even applicability to the work of other academics who are better positioned to translate the theories to the practical realm. We refer to the lack of relevance, accessibility, and applicability of psychological theory to the rest of society as the practicality crisis. The practicality crisis harms the field in its ability to attract the next generation of scholars and maintain viability at the national level. We describe practical theory and illustrate its use in the field of self-regulation. Psychological theory is historically and scientifically well positioned to become useful should scholars in the field decide to value practicality. We offer a set of incentives to encourage the return of social psychology to the Lewinian vision of a useful science that speaks to pressing social issues.","author":[{"family":"Berkman","given":"Elliot T"},{"family":"Wilson","given":"Sylas M"}],"citation-key":"berkmanUsefulGoodTheory","language":"en","source":"Zotero","title":"So Useful as a Good Theory? The Practicality Crisis in (Social) Psychological Theory","type":"article-journal"},{"id":"burdissoIDIAPersCausalNews2022","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Burdisso","given":"Sergio"},{"family":"Zuluaga-gomez","given":"Juan Pablo"},{"family":"Villatoro-tello","given":"Esau"},{"family":"Fajcik","given":"Martin"},{"family":"Singh","given":"Muskaan"},{"family":"Smrz","given":"Pavel"},{"family":"Motlicek","given":"Petr"}],"citation-key":"burdissoIDIAPersCausalNews2022","container-title":"Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)","DOI":"10.18653/v1/2022.case-1.9","event-place":"Abu Dhabi, United Arab Emirates (Hybrid)","event-title":"Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)","issued":{"date-parts":[["2022"]]},"language":"en","page":"61-69","publisher":"Association for Computational Linguistics","publisher-place":"Abu Dhabi, United Arab Emirates (Hybrid)","source":"DOI.org (Crossref)","title":"IDIAPers @ Causal News Corpus 2022: Efficient Causal Relation Identification Through a Prompt-based Few-shot Approach","title-short":"IDIAPers @ Causal News Corpus 2022","type":"paper-conference","URL":"https://aclanthology.org/2022.case-1.9"},{"id":"busettiCausalityGoodPractice2023","abstract":"Relevance to practice is an open issue for scholars in public policy and public administration. One major problem is the need to produce knowledge that can guide practitioners designing and implementing public interventions in specific contexts. This article claims that investigating the causal mechanisms of policy programs—i.e., modeling why and how they produce outcomes—can contribute to such knowledge. In this regard, mechanisms offer essential information to guide practitioners when replicating, adjusting, and designing interventions. Unfortunately, not all models of mechanisms can inform practice. The article proposes a strategy for design research and practice inspired by reverse engineering: selecting successful programs, causal modeling, assessing the target context, and designing. Scholars should model mechanisms by identifying the program and non-program elements that contribute to the outcome of interest and abstracting their causal powers. Practitioners can use these models, diagnose their target context, and adjust designs to deal with context-specific problems. The proposed research agenda may enhance orientation to practice and offer a middle ground between the search for abstract, general relationships, and single-case analyses.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Busetti","given":"Simone"}],"citation-key":"busettiCausalityGoodPractice2023","container-title":"Policy Sciences","container-title-short":"Policy Sci","DOI":"10.1007/s11077-023-09493-7","ISSN":"0032-2687, 1573-0891","issue":"2","issued":{"date-parts":[["2023",6]]},"language":"en","page":"419-438","source":"DOI.org (Crossref)","title":"Causality is good for practice: policy design and reverse engineering","title-short":"Causality is good for practice","type":"article-journal","URL":"https://link.springer.com/10.1007/s11077-023-09493-7","volume":"56"},{"id":"chen2021better","author":[{"family":"Chen","given":"Zheng"},{"family":"Zhang","given":"Yunchen"}],"citation-key":"chen2021better","container-title":"Artificial neural networks and machine Learning–ICANN 2021: 30th international conference on artificial neural networks, bratislava, slovakia, september 14–17, 2021, proceedings, part II 30","issued":{"date-parts":[["2021"]]},"page":"537–548","publisher":"Springer","title":"Better few-shot text classification with pre-trained language model","type":"paper-conference"},{"id":"devlinBERTPretrainingDeep","abstract":"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.","author":[{"family":"Devlin","given":"Jacob"},{"family":"Chang","given":"Ming-Wei"},{"family":"Lee","given":"Kenton"},{"family":"Toutanova","given":"Kristina"}],"citation-key":"devlinBERTPretrainingDeep","language":"en","source":"Zotero","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","type":"article-journal"},{"id":"dingParameterefficientFinetuningLargescale2023","abstract":"Abstract\n            With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Ding","given":"Ning"},{"family":"Qin","given":"Yujia"},{"family":"Yang","given":"Guang"},{"family":"Wei","given":"Fuchao"},{"family":"Yang","given":"Zonghan"},{"family":"Su","given":"Yusheng"},{"family":"Hu","given":"Shengding"},{"family":"Chen","given":"Yulin"},{"family":"Chan","given":"Chi-Min"},{"family":"Chen","given":"Weize"},{"family":"Yi","given":"Jing"},{"family":"Zhao","given":"Weilin"},{"family":"Wang","given":"Xiaozhi"},{"family":"Liu","given":"Zhiyuan"},{"family":"Zheng","given":"Hai-Tao"},{"family":"Chen","given":"Jianfei"},{"family":"Liu","given":"Yang"},{"family":"Tang","given":"Jie"},{"family":"Li","given":"Juanzi"},{"family":"Sun","given":"Maosong"}],"citation-key":"dingParameterefficientFinetuningLargescale2023","container-title":"Nature Machine Intelligence","container-title-short":"Nat Mach Intell","DOI":"10.1038/s42256-023-00626-4","ISSN":"2522-5839","issue":"3","issued":{"date-parts":[["2023",3,2]]},"language":"en","page":"220-235","source":"DOI.org (Crossref)","title":"Parameter-efficient fine-tuning of large-scale pre-trained language models","type":"article-journal","URL":"https://www.nature.com/articles/s42256-023-00626-4","volume":"5"},{"id":"doccano","author":[{"family":"Nakayama","given":"Hiroki"},{"family":"Kubo","given":"Takahiro"},{"family":"Kamura","given":"Junya"},{"family":"Taniguchi","given":"Yasufumi"},{"family":"Liang","given":"Xu"}],"citation-key":"doccano","issued":{"date-parts":[["2018"]]},"title":"<span class=\"nocase\">doccano</span>: Text annotation tool for human","type":"document","URL":"https://github.com/doccano/doccano"},{"id":"fleissMeasuringNominalScale1971","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Fleiss","given":"Joseph L."}],"citation-key":"fleissMeasuringNominalScale1971","container-title":"Psychological Bulletin","container-title-short":"Psychological Bulletin","DOI":"10.1037/h0031619","ISSN":"1939-1455, 0033-2909","issue":"5","issued":{"date-parts":[["1971",11]]},"language":"en","page":"378-382","source":"DOI.org (Crossref)","title":"Measuring nominal scale agreement among many raters.","type":"article-journal","URL":"https://doi.apa.org/doi/10.1037/h0031619","volume":"76"},{"id":"frattiniAutomaticExtractionCauseeffectrelations2020","abstract":"Background: The detection and extraction of causality from natural language sentences have shown great potential in various fields of application. The field of requirements engineering is eligible for multiple reasons: (1) requirements artifacts are primarily written in natural language, (2) causal sentences convey essential context about the subject of requirements, and (3) extracted and formalized causality relations are usable for a (semi-)automatic translation into further artifacts, such as test cases.\nObjective: We aim at understanding the value of interactive causality extraction based on syntactic criteria for the context of requirements engineering.\nMethod: We developed a prototype of a system for automatic causality extraction and evaluate it by applying it to a set of publicly available requirements artifacts, determining whether the automatic extraction reduces the manual effort of requirements formalization.\nResult: During the evaluation we analyzed 4457 natural language sentences from 18 requirements documents, 558 of which were causal (12.52%). The best evaluation of a requirements document provided an automatic extraction of 48.57% cause-effect graphs on average, which demonstrates the feasibility of the approach. Limitation: The feasibility of the approach has been proven in theory but lacks exploration of being scaled up for practical use. Evaluating the applicability of the automatic causality extraction for a requirements engineer is left for future research.\nConclusion: A syntactic approach for causality extraction is viable for the context of requirements engineering and can aid a pipeline towards an automatic generation of further artifacts from requirements artifacts.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Frattini","given":"Julian"},{"family":"Junker","given":"Maximilian"},{"family":"Unterkalmsteiner","given":"Michael"},{"family":"Mendez","given":"Daniel"}],"citation-key":"frattiniAutomaticExtractionCauseeffectrelations2020","container-title":"Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering","DOI":"10.1145/3324884.3416549","event-place":"Virtual Event Australia","event-title":"ASE '20: 35th IEEE/ACM International Conference on Automated Software Engineering","ISBN":"978-1-4503-6768-4","issued":{"date-parts":[["2020",12,21]]},"language":"en","page":"561-572","publisher":"ACM","publisher-place":"Virtual Event Australia","source":"DOI.org (Crossref)","title":"Automatic extraction of cause-effect-relations from requirements artifacts","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3324884.3416549"},{"id":"gaoMakingPretrainedLanguage2021","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Gao","given":"Tianyu"},{"family":"Fisch","given":"Adam"},{"family":"Chen","given":"Danqi"}],"citation-key":"gaoMakingPretrainedLanguage2021","container-title":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)","DOI":"10.18653/v1/2021.acl-long.295","event-place":"Online","event-title":"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)","issued":{"date-parts":[["2021"]]},"language":"en","page":"3816-3830","publisher":"Association for Computational Linguistics","publisher-place":"Online","source":"DOI.org (Crossref)","title":"Making Pre-trained Language Models Better Few-shot Learners","type":"paper-conference","URL":"https://aclanthology.org/2021.acl-long.295"},{"id":"grossStructureCausalChains2018","abstract":"Sociologists are increasingly attentive to the mechanisms responsible for cause-and-effect relationships in the social world. But an aspect of mechanistic causality has not been sufficiently considered. It is well recognized that most phenomena of interest to social science result from multiple mechanisms operating in sequence. However, causal chains—sequentially linked mechanisms and their enabling background conditions—vary not just substantively, by the kind of causal work they do, but also structurally, by their formal properties. In this article, the author examines the nature of causal chains, identifies major structural dimensions along which they differ, and makes a case that a mechanism-based explanation would be enhanced if causal chains and their structures were brought to the analytical forefront.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Gross","given":"Neil"}],"citation-key":"grossStructureCausalChains2018","container-title":"Sociological Theory","container-title-short":"Sociological Theory","DOI":"10.1177/0735275118811377","ISSN":"0735-2751, 1467-9558","issue":"4","issued":{"date-parts":[["2018",12]]},"language":"en","page":"343-367","source":"DOI.org (Crossref)","title":"The Structure of Causal Chains","type":"article-journal","URL":"http://journals.sagepub.com/doi/10.1177/0735275118811377","volume":"36"},{"id":"groszTabooExplicitCausal","abstract":"Causal inference is a central goal of research. However, most psychologists refrain from explicitly addressing causal research questions and avoid drawing causal inference on the basis of nonexperimental evidence. We argue that this taboo against causal inference in nonexperimental psychology impairs study design and data analysis, holds back cumulative research, leads to a disconnect between original findings and how they are interpreted in subsequent work, and limits the relevance of nonexperimental psychology for policymaking. At the same time, the taboo does not prevent researchers from interpreting findings as causal effects—the inference is simply made implicitly, and assumptions remain unarticulated. Thus, we recommend that nonexperimental psychologists begin to talk openly about causal assumptions and causal effects. Only then can researchers take advantage of recent methodological advances in causal reasoning and analysis and develop a solid understanding of the underlying causal mechanisms that can inform future research, theory, and policymakers.","author":[{"family":"Grosz","given":"Michael P"},{"family":"Rohrer","given":"Julia M"},{"family":"Thoemmes","given":"Felix"}],"citation-key":"groszTabooExplicitCausal","language":"en","source":"Zotero","title":"The Taboo Against Explicit Causal Inference in Nonexperimental Psychology","type":"article-journal"},{"id":"hassanzadehCausalKnowledgeExtraction2020","abstract":"In this demonstration, we present a system for mining causal knowledge from large corpuses of text documents, such as millions of news articles. Our system provides a collection of APIs for causal analysis and retrieval. These APIs enable searching for the effects of a given cause and the causes of a given effect, as well as the analysis of existence of causal relation given a pair of phrases. The analysis includes a score that indicates the likelihood of the existence of a causal relation. It also provides evidence from an input corpus supporting the existence of a causal relation between input phrases. Our system uses generic unsupervised and weakly supervised methods of causal relation extraction that do not impose semantic constraints on causes and effects. We show example use cases developed for a commercial application in enterprise risk management.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Hassanzadeh","given":"Oktie"},{"family":"Bhattacharjya","given":"Debarun"},{"family":"Feblowitz","given":"Mark"},{"family":"Srinivas","given":"Kavitha"},{"family":"Perrone","given":"Michael"},{"family":"Sohrabi","given":"Shirin"},{"family":"Katz","given":"Michael"}],"citation-key":"hassanzadehCausalKnowledgeExtraction2020","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","container-title-short":"AAAI","DOI":"10.1609/aaai.v34i09.7092","ISSN":"2374-3468, 2159-5399","issue":"09","issued":{"date-parts":[["2020",4,3]]},"language":"en","page":"13610-13611","source":"DOI.org (Crossref)","title":"Causal Knowledge Extraction through Large-Scale Text Mining","type":"article-journal","URL":"https://ojs.aaai.org/index.php/AAAI/article/view/7092","volume":"34"},{"id":"hedstromCausalMechanismsSocial2010","abstract":"During the past decade, social mechanisms and mechanism-based explanations have received considerable attention in the social sciences as well as in the philosophy of science. This article critically reviews the most important philosophical and social science contributions to the mechanism approach. The ﬁrst part discusses the idea of mechanismbased explanation from the point of view of philosophy of science and relates it to causation and to the covering-law account of explanation. The second part focuses on how the idea of mechanisms has been used in the social sciences. The ﬁnal part discusses recent developments in analytical sociology, covering the nature of sociological explananda, the role of theory of action in mechanism-based explanations, Merton’s idea of middle-range theory, and the role of agent-based simulations in the development of mechanism-based explanations.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Hedström","given":"Peter"},{"family":"Ylikoski","given":"Petri"}],"citation-key":"hedstromCausalMechanismsSocial2010","container-title":"Annual Review of Sociology","container-title-short":"Annu. Rev. Sociol.","DOI":"10.1146/annurev.soc.012809.102632","ISSN":"0360-0572, 1545-2115","issue":"1","issued":{"date-parts":[["2010",6,1]]},"language":"en","page":"49-67","source":"DOI.org (Crossref)","title":"Causal Mechanisms in the Social Sciences","type":"article-journal","URL":"https://www.annualreviews.org/doi/10.1146/annurev.soc.012809.102632","volume":"36"},{"id":"huggingfaceTransformersDocumentation2024","accessed":{"date-parts":[["2024",2,23]]},"author":[{"literal":"Hugging Face"}],"citation-key":"huggingfaceTransformersDocumentation2024","container-title":"Transformers","issued":{"date-parts":[["2024"]]},"title":"Transformers Documentation","type":"webpage","URL":"https://huggingface.co/docs/transformers/index"},{"id":"jiangMistral7B2023","abstract":"We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Jiang","given":"Albert Q."},{"family":"Sablayrolles","given":"Alexandre"},{"family":"Mensch","given":"Arthur"},{"family":"Bamford","given":"Chris"},{"family":"Chaplot","given":"Devendra Singh"},{"family":"Casas","given":"Diego","dropping-particle":"de las"},{"family":"Bressand","given":"Florian"},{"family":"Lengyel","given":"Gianna"},{"family":"Lample","given":"Guillaume"},{"family":"Saulnier","given":"Lucile"},{"family":"Lavaud","given":"Lélio Renard"},{"family":"Lachaux","given":"Marie-Anne"},{"family":"Stock","given":"Pierre"},{"family":"Scao","given":"Teven Le"},{"family":"Lavril","given":"Thibaut"},{"family":"Wang","given":"Thomas"},{"family":"Lacroix","given":"Timothée"},{"family":"Sayed","given":"William El"}],"citation-key":"jiangMistral7B2023","issued":{"date-parts":[["2023",10,10]]},"language":"en","number":"arXiv:2310.06825","publisher":"arXiv","source":"arXiv.org","title":"Mistral 7B","type":"article","URL":"http://arxiv.org/abs/2310.06825"},{"id":"landisMeasurementObserverAgreement1977","abstract":"This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Landis","given":"J. Richard"},{"family":"Koch","given":"Gary G."}],"citation-key":"landisMeasurementObserverAgreement1977","container-title":"Biometrics","container-title-short":"Biometrics","DOI":"10.2307/2529310","ISSN":"0006341X","issue":"1","issued":{"date-parts":[["1977",3]]},"language":"en","page":"159","source":"DOI.org (Crossref)","title":"The Measurement of Observer Agreement for Categorical Data","type":"article-journal","URL":"https://www.jstor.org/stable/2529310?origin=crossref","volume":"33"},{"id":"liuRoBERTaRobustlyOptimized2019","abstract":"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Liu","given":"Yinhan"},{"family":"Ott","given":"Myle"},{"family":"Goyal","given":"Naman"},{"family":"Du","given":"Jingfei"},{"family":"Joshi","given":"Mandar"},{"family":"Chen","given":"Danqi"},{"family":"Levy","given":"Omer"},{"family":"Lewis","given":"Mike"},{"family":"Zettlemoyer","given":"Luke"},{"family":"Stoyanov","given":"Veselin"}],"citation-key":"liuRoBERTaRobustlyOptimized2019","issued":{"date-parts":[["2019",7,26]]},"language":"en","number":"arXiv:1907.11692","publisher":"arXiv","source":"arXiv.org","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach","title-short":"RoBERTa","type":"article","URL":"http://arxiv.org/abs/1907.11692"},{"id":"lopez2009grobid","author":[{"family":"Lopez","given":"Patrice"}],"citation-key":"lopez2009grobid","container-title":"Research and advanced technology for digital libraries: 13th european conference, ECDL 2009, corfu, greece, september 27-October 2, 2009. Proceedings 13","issued":{"date-parts":[["2009"]]},"page":"473–474","publisher":"Springer","title":"GROBID: Combining automatic bibliographic data recognition and term extraction for scholarship publications","type":"paper-conference"},{"id":"maWhatHowExtracting2023","abstract":"In response to the exponential growth of the volume of scientific publications, researchers have proposed a multitude of information extraction methods for extracting entities and relations, such as task, dataset, metric, and method entities. However, the existing methods cannot directly provide readers with procedural scientific information that demonstrates the path to the prob­ lem’s solution. From the perspective of applied science, we propose a novel schema for the applied AI community, namely a metric-driven mechanism schema (Operation, Effect, Direc­ tion). Our schema depicts the procedural scientific information concerning “How to optimize the quantitative metrics for a specific task?” In this paper, we choose papers in the domain of NLP for our study, which is a representative branch of Artificial Intelligence (AI). Specifically, we first construct a dataset that covers the metric-driven mechanisms in single and multiple sentences. Then we propose a framework for extracting metric-driven mechanisms, which includes three sub-models: 1) a mechanism detection model, 2) a query-guided seq2seq mechanism extraction model, and 3) a task recognition model. Finally, a metric-driven mechanism knowledge graph, named MKGNLP, is constructed. Our MKGNLP has over 43K n-ary mechanism relations in the form of (Operation, Effect, Direction, Task). The human evaluation shows that the extracted metricdriven mechanisms in MKGNLP achieve 81.4% accuracy. Our model also shows the potential for creating applications to assist applied AI scientists to solve specific problems.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Ma","given":"Yongqiang"},{"family":"Liu","given":"Jiawei"},{"family":"Lu","given":"Wei"},{"family":"Cheng","given":"Qikai"}],"citation-key":"maWhatHowExtracting2023","container-title":"Information Processing & Management","container-title-short":"Information Processing & Management","DOI":"10.1016/j.ipm.2023.103315","ISSN":"03064573","issue":"3","issued":{"date-parts":[["2023",5]]},"language":"en","page":"103315","source":"DOI.org (Crossref)","title":"From “what” to “how”: Extracting the Procedural Scientific Information Toward the Metric-optimization in AI","title-short":"From “what” to “how”","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0306457323000523","volume":"60"},{"id":"meehlAppraisingAmendingTheories1990","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Meehl","given":"Paul E."}],"citation-key":"meehlAppraisingAmendingTheories1990","container-title":"Psychological Inquiry","container-title-short":"Psychological Inquiry","DOI":"10.1207/s15327965pli0102_1","ISSN":"1047-840X, 1532-7965","issue":"2","issued":{"date-parts":[["1990",4]]},"language":"en","page":"108-141","source":"DOI.org (Crossref)","title":"Appraising and Amending Theories: The Strategy of Lakatosian Defense and Two Principles that Warrant It","title-short":"Appraising and Amending Theories","type":"article-journal","URL":"http://www.tandfonline.com/doi/abs/10.1207/s15327965pli0102_1","volume":"1"},{"id":"moghimifarDomainAdaptativeCausality","abstract":"Automated discovery of causal relationships from text is a challenging task. Current approaches which are mainly based on the extraction of low-level relations among individual events are limited by the shortage of publicly available labelled data. Therefore, the resulting models perform poorly when applied to a distributionally different domain for which labelled data did not exist at the time of training. To overcome this limitation, in this paper, we leverage the characteristics of dependency trees and adversarial learning to address the tasks of adaptive causality identiﬁcation and localisation. The term adaptive is used since the training and test data come from two distributionally different datasets, which to the best of our knowledge, this work is the ﬁrst to address. Moreover, we present a new causality dataset, namely MEDCAUS1, which integrates all types of causality in the text. Our experiments on four different benchmark causality datasets demonstrate the superiority of our approach over the existing baselines, by up to 7% improvement, on the tasks of identiﬁcation and localisation of the causal relations from the text.","author":[{"family":"Moghimifar","given":"Farhad"},{"family":"Haffari","given":"Gholamreza"},{"family":"Baktashmotlagh","given":"Mahsa"}],"citation-key":"moghimifarDomainAdaptativeCausality","language":"en","source":"Zotero","title":"Domain Adaptative Causality Encoder","type":"article-journal"},{"id":"morley2014academic","author":[{"family":"Morley","given":"John"}],"citation-key":"morley2014academic","container-title":"Manchester: University of Manchester","issued":{"date-parts":[["2014"]]},"title":"Academic phrasebank","type":"article-journal"},{"id":"morrisonMethodsUnderstandingsExpressions2020","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Morrison","given":"Keith"},{"family":"Van Der Werf","given":"Greetje"}],"citation-key":"morrisonMethodsUnderstandingsExpressions2020","container-title":"Educational Research and Evaluation","container-title-short":"Educational Research and Evaluation","DOI":"10.1080/13803611.2021.1991643","ISSN":"1380-3611, 1744-4187","issue":"7-8","issued":{"date-parts":[["2020",11,16]]},"language":"en","page":"339-343","source":"DOI.org (Crossref)","title":"Methods, understandings, and expressions of causality in educational research","type":"article-journal","URL":"https://www.tandfonline.com/doi/full/10.1080/13803611.2021.1991643","volume":"26"},{"id":"nakayamaDoccanoTextAnnotation2018","author":[{"family":"Nakayama","given":"Hiroki"},{"family":"Kubo","given":"Takahiro"},{"family":"Kamura","given":"Junya"},{"family":"Taniguchi","given":"Yasufumi"},{"family":"Liang","given":"Xu"}],"citation-key":"nakayamaDoccanoTextAnnotation2018","issued":{"date-parts":[["2018"]]},"title":"doccano: Text Annotation Tool for Human","type":"document","URL":"https://github.com/doccano/doccano"},{"id":"ottHedgingWeaselWords2018","abstract":"Background and Objectives: Words in scientific discourse must be truthful. Introducing ambiguity or creating a false narrative by insinuating close counts or almost statements as facts that appeal to a truth the writer wants to exist doesn’t make it true. A reader’s personal interpretation because of hedging or weasel words creates an opportunity for truthiness as a belief to become a fact when it isn’t.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Ott","given":"Douglas E."}],"citation-key":"ottHedgingWeaselWords2018","container-title":"JSLS : Journal of the Society of Laparoendoscopic Surgeons","container-title-short":"JSLS","DOI":"10.4293/JSLS.2018.00063","ISSN":"1086-8089, 1938-3797","issue":"4","issued":{"date-parts":[["2018"]]},"language":"en","page":"e2018.00063","source":"DOI.org (Crossref)","title":"Hedging, Weasel Words, and Truthiness in Scientific Writing","type":"article-journal","URL":"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6311890/","volume":"22"},{"id":"paszke2017automatic","author":[{"family":"Paszke","given":"Adam"},{"family":"Gross","given":"Sam"},{"family":"Chintala","given":"Soumith"},{"family":"Chanan","given":"Gregory"},{"family":"Yang","given":"Edward"},{"family":"DeVito","given":"Zachary"},{"family":"Lin","given":"Zeming"},{"family":"Desmaison","given":"Alban"},{"family":"Antiga","given":"Luca"},{"family":"Lerer","given":"Adam"}],"citation-key":"paszke2017automatic","issued":{"date-parts":[["2017"]]},"title":"Automatic differentiation in pytorch","type":"article-journal"},{"id":"paszkeAutomaticDifferentiationPyTorch2017","author":[{"family":"Paszke","given":"Adam"},{"family":"Gross","given":"Sam"},{"family":"Chintala","given":"Soumith"},{"family":"Chanan","given":"Gregory"},{"family":"Yang","given":"Edward"},{"family":"DeVito","given":"Zachary"},{"family":"Lin","given":"Zeming"},{"family":"Desmaison","given":"Alban"},{"family":"Antiga","given":"Luca"},{"family":"Lerer","given":"Adam"}],"citation-key":"paszkeAutomaticDifferentiationPyTorch2017","issued":{"date-parts":[["2017"]]},"title":"Automatic differentiation in PyTorch","type":"article-journal"},{"id":"pearl2009causal","author":[{"family":"Pearl","given":"Judea"}],"citation-key":"pearl2009causal","issued":{"date-parts":[["2009"]]},"title":"Causal inference in statistics: An overview","type":"article-journal"},{"id":"reiterTheoryMethodologyExploratory","abstract":"Confirmatory, deductive research cannot produce absolute truths, according Karl POPPER (2002). If we accept this premise, then it is worth giving inductive and explorative research another chance. Exploration can produce valid and insightful findings in the social sciences, if conducted in a transparent and self-reflexive way. It can also profit from applying dialectical thinking. This article proposes a rationale for exploration in the social sciences and it elaborates the criteria on which such research must stand.","author":[{"family":"Reiter","given":"Bernd"}],"citation-key":"reiterTheoryMethodologyExploratory","language":"en","source":"Zotero","title":"Theory and Methodology of Exploratory Social Science Research","type":"article-journal","volume":"5"},{"id":"riazBetterUnderstandingCausality","abstract":"The identiﬁcation of causal relations between verbal events is important for achieving natural language understanding. However, the problem has proven notoriously difﬁcult since it is not clear which types of knowledge are necessary to solve this challenging problem close to human level performance. Instead of employing a large set of features proved useful in other NLP tasks, we split the problem in smaller sub problems. Since verbs play a very important role in causal relations, in this paper we harness, explore, and evaluate the predictive power of causal associations of verb-verb pairs. More speciﬁcally, we propose a set of knowledge-rich metrics to learn the likelihood of causal relations between verbs. Employing these metrics, we automatically generate a knowledge base (KBc) which identiﬁes three categories of verb pairs: Strongly Causal, Ambiguous, and Strongly Non-causal. The knowledge base is evaluated empirically. The results show that our metrics perform significantly better than the state-of-the-art on the task of detecting causal verbal events.","author":[{"family":"Riaz","given":"Mehwish"},{"family":"Girju","given":"Roxana"}],"citation-key":"riazBetterUnderstandingCausality","language":"en","source":"Zotero","title":"Toward a Better Understanding of Causality between Verbal Events: Extraction and Analysis of the Causal Power of Verb-Verb Associations","type":"article-journal"},{"id":"sanbonmatsuImpactComplexityMethods2021","abstract":"The study of human behavior is severely hampered by logistical problems, ethical and legal constraints, and funding shortfalls. However, the biggest difficulty of conducting social and behavioral research is the extraordinary complexity of the study phenomena. In this article, we review the impact of complexity on research design, hypothesis testing, measurement, data analyses, reproducibility, and the communication of findings in psychological science. The systematic investigation of the world often requires different approaches because of the variability in complexity. Confirmatory testing, multi-factorial designs, survey methods, large samples, and modeling are frequently needed to study complex social and behavioral topics. Complexity impedes the measurement of general constructs, the reproducibility of results and scientific reporting, and the general rigor of research. Many of the benchmarks established by classic work in physical science are not attainable in studies of more complex phenomena. Consequently, the standards used to evaluate scientific research should be tethered to the complexity of the study topic.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Sanbonmatsu","given":"David M."},{"family":"Cooley","given":"Emily H."},{"family":"Butner","given":"Jonathan E."}],"citation-key":"sanbonmatsuImpactComplexityMethods2021","container-title":"Frontiers in Psychology","container-title-short":"Front. Psychol.","DOI":"10.3389/fpsyg.2020.580111","ISSN":"1664-1078","issued":{"date-parts":[["2021",1,21]]},"language":"en","page":"580111","source":"DOI.org (Crossref)","title":"The Impact of Complexity on Methods and Findings in Psychological Science","type":"article-journal","URL":"https://www.frontiersin.org/articles/10.3389/fpsyg.2020.580111/full","volume":"11"},{"id":"spadaroCooperationDatabankMachineReadable","abstract":"Publishing studies using standardized, machine-readable formats will enable machines to perform meta-analyses on demand. To build a semantically enhanced technology that embodies these functions, we developed the Cooperation Databank (CoDa)—a databank that contains 2,636 studies on human cooperation (1958–2017) conducted in 78 societies involving 356,283 participants. Experts annotated these studies along 312 variables, including the quantitative results (13,959 effects). We designed an ontology that defines and relates concepts in cooperation research and that can represent the relationships between results of correlational and experimental studies. We have created a research platform that, given the data set, enables users to retrieve studies that test the relation of variables with cooperation, visualize these study results, and perform (a) meta-analyses, (b) metaregressions, (c) estimates of publication bias, and (d) statistical power analyses for future studies. We leveraged the data set with visualization tools that allow users to explore the ontology of concepts in cooperation research and to plot a citation network of the history of studies. CoDa offers a vision of how publishing studies in a machine-readable format can establish institutions and tools that improve scientific practices and knowledge.","author":[{"family":"Spadaro","given":"Giuliana"},{"family":"Tiddi","given":"Ilaria"},{"family":"Columbus","given":"Simon"},{"family":"Jin","given":"Shuxian"}],"citation-key":"spadaroCooperationDatabankMachineReadable","container-title":"Perspectives on Psychological Science","language":"en","source":"Zotero","title":"The Cooperation Databank: Machine-Readable Science Accelerates Research Synthesis","type":"article-journal"},{"id":"tanEventCausalityIdentification2022","abstract":"The Event Causality Identification Shared Task of CASE 2022 involved two subtasks working on the Causal News Corpus. Subtask 1 required participants to predict if a sentence contains a causal relation or not. This is a supervised binary classification task. Subtask 2 required participants to identify the Cause, Effect and Signal spans per causal sentence. This could be seen as a supervised sequence labeling task. For both subtasks, participants uploaded their predictions for a held-out test set, and ranking was done based on binary F1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes the work of the 17 teams that submitted their results to our competition and 12 system description papers that were received. The best F1 scores achieved for Subtask 1 and 2 were 86.19% and 54.15%, respectively. All the top-performing approaches involved pretrained language models fine-tuned to the targeted task. We further discuss these approaches and analyze errors across participants’ systems in this paper.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Tan","given":"Fiona Anting"},{"family":"Hettiarachchi","given":"Hansi"},{"family":"Hürriyetoğlu","given":"Ali"},{"family":"Caselli","given":"Tommaso"},{"family":"Uca","given":"Onur"},{"family":"Liza","given":"Farhana Ferdousi"},{"family":"Oostdijk","given":"Nelleke"}],"citation-key":"tanEventCausalityIdentification2022","container-title":"Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)","DOI":"10.18653/v1/2022.case-1.28","event-place":"Abu Dhabi, United Arab Emirates (Hybrid)","event-title":"Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE)","issued":{"date-parts":[["2022"]]},"language":"en","page":"195-208","publisher":"Association for Computational Linguistics","publisher-place":"Abu Dhabi, United Arab Emirates (Hybrid)","source":"DOI.org (Crossref)","title":"Event Causality Identification with Causal News Corpus - Shared Task 3, CASE 2022","type":"paper-conference","URL":"https://aclanthology.org/2022.case-1.28"},{"id":"tanUniCausalUnifiedBenchmark2023","abstract":"Current causal text mining datasets vary in objectives, data coverage, and annotation schemes. These inconsistent eﬀorts prevent modeling capabilities and fair comparisons of model performance. Furthermore, few datasets include cause-eﬀect span annotations, which are needed for end-to-end causal relation extraction. To address these issues, we propose UniCausal, a uniﬁed benchmark for causal text mining across three tasks: (I) Causal Sequence Classiﬁcation, (II) Cause-Eﬀect Span Detection and (III) Causal Pair Classiﬁcation. We consolidated and aligned annotations of six high quality, mainly human-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165 examples for each task respectively. Since the deﬁnition of causality can be subjective, our framework was designed to allow researchers to work on some or all datasets and tasks. To create an initial benchmark, we ﬁne-tuned BERT pre-trained language models to each task, achieving 70.10% Binary F1, 52.42% Macro F1, and 84.68% Binary F1 scores respectively.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Tan","given":"Fiona Anting"},{"family":"Zuo","given":"Xinyu"},{"family":"Ng","given":"See-Kiong"}],"citation-key":"tanUniCausalUnifiedBenchmark2023","issued":{"date-parts":[["2023",4,14]]},"language":"en","number":"arXiv:2208.09163","publisher":"arXiv","source":"arXiv.org","title":"UniCausal: Unified Benchmark and Repository for Causal Text Mining","title-short":"UniCausal","type":"article","URL":"http://arxiv.org/abs/2208.09163"},{"id":"thapaBeingHonestCausal2020","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Thapa","given":"Deependra K."},{"family":"Visentin","given":"Denis C."},{"family":"Hunt","given":"Glenn E."},{"family":"Watson","given":"Roger"},{"family":"Cleary","given":"Michelle"}],"citation-key":"thapaBeingHonestCausal2020","container-title":"Journal of Advanced Nursing","container-title-short":"Journal of Advanced Nursing","DOI":"10.1111/jan.14311","ISSN":"0309-2402, 1365-2648","issue":"6","issued":{"date-parts":[["2020",6]]},"language":"en","page":"1285-1288","source":"DOI.org (Crossref)","title":"Being honest with causal language in writing for publication","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/10.1111/jan.14311","volume":"76"},{"id":"touvronLlamaOpenFoundation2023","abstract":"In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Touvron","given":"Hugo"},{"family":"Martin","given":"Louis"},{"family":"Stone","given":"Kevin"},{"family":"Albert","given":"Peter"},{"family":"Almahairi","given":"Amjad"},{"family":"Babaei","given":"Yasmine"},{"family":"Bashlykov","given":"Nikolay"},{"family":"Batra","given":"Soumya"},{"family":"Bhargava","given":"Prajjwal"},{"family":"Bhosale","given":"Shruti"},{"family":"Bikel","given":"Dan"},{"family":"Blecher","given":"Lukas"},{"family":"Ferrer","given":"Cristian Canton"},{"family":"Chen","given":"Moya"},{"family":"Cucurull","given":"Guillem"},{"family":"Esiobu","given":"David"},{"family":"Fernandes","given":"Jude"},{"family":"Fu","given":"Jeremy"},{"family":"Fu","given":"Wenyin"},{"family":"Fuller","given":"Brian"},{"family":"Gao","given":"Cynthia"},{"family":"Goswami","given":"Vedanuj"},{"family":"Goyal","given":"Naman"},{"family":"Hartshorn","given":"Anthony"},{"family":"Hosseini","given":"Saghar"},{"family":"Hou","given":"Rui"},{"family":"Inan","given":"Hakan"},{"family":"Kardas","given":"Marcin"},{"family":"Kerkez","given":"Viktor"},{"family":"Khabsa","given":"Madian"},{"family":"Kloumann","given":"Isabel"},{"family":"Korenev","given":"Artem"},{"family":"Koura","given":"Punit Singh"},{"family":"Lachaux","given":"Marie-Anne"},{"family":"Lavril","given":"Thibaut"},{"family":"Lee","given":"Jenya"},{"family":"Liskovich","given":"Diana"},{"family":"Lu","given":"Yinghai"},{"family":"Mao","given":"Yuning"},{"family":"Martinet","given":"Xavier"},{"family":"Mihaylov","given":"Todor"},{"family":"Mishra","given":"Pushkar"},{"family":"Molybog","given":"Igor"},{"family":"Nie","given":"Yixin"},{"family":"Poulton","given":"Andrew"},{"family":"Reizenstein","given":"Jeremy"},{"family":"Rungta","given":"Rashi"},{"family":"Saladi","given":"Kalyan"},{"family":"Schelten","given":"Alan"},{"family":"Silva","given":"Ruan"},{"family":"Smith","given":"Eric Michael"},{"family":"Subramanian","given":"Ranjan"},{"family":"Tan","given":"Xiaoqing Ellen"},{"family":"Tang","given":"Binh"},{"family":"Taylor","given":"Ross"},{"family":"Williams","given":"Adina"},{"family":"Kuan","given":"Jian Xiang"},{"family":"Xu","given":"Puxin"},{"family":"Yan","given":"Zheng"},{"family":"Zarov","given":"Iliyan"},{"family":"Zhang","given":"Yuchen"},{"family":"Fan","given":"Angela"},{"family":"Kambadur","given":"Melanie"},{"family":"Narang","given":"Sharan"},{"family":"Rodriguez","given":"Aurelien"},{"family":"Stojnic","given":"Robert"},{"family":"Edunov","given":"Sergey"},{"family":"Scialom","given":"Thomas"}],"citation-key":"touvronLlamaOpenFoundation2023","issued":{"date-parts":[["2023",7,19]]},"language":"en","number":"arXiv:2307.09288","publisher":"arXiv","source":"arXiv.org","title":"Llama 2: Open Foundation and Fine-Tuned Chat Models","title-short":"Llama 2","type":"article","URL":"http://arxiv.org/abs/2307.09288"},{"id":"vandevenNothingQuitePractical1989","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Van De Ven","given":"Andrew H."}],"citation-key":"vandevenNothingQuitePractical1989","container-title":"Academy of Management Review","container-title-short":"AMR","DOI":"10.5465/amr.1989.4308370","ISSN":"0363-7425, 1930-3807","issue":"4","issued":{"date-parts":[["1989",10]]},"language":"en","page":"486-489","source":"DOI.org (Crossref)","title":"Nothing Is Quite So Practical as a Good Theory","type":"article-journal","URL":"http://journals.aom.org/doi/10.5465/amr.1989.4308370","volume":"14"},{"id":"vanlissaMappingPhenomenaRelevant2022","abstract":"Adolescence is a developmentally sensitive period for emotion regulation with potentially lifelong implications for mental health and well-being. Although substantial empirical research has addressed this topic, the literature is fragmented across subdisciplines, and an overarching theoretical framework is lacking. The first step toward constructing a unifying framework is identifying relevant phenomena. This systematic review of 6305 articles used text mining to identify phenomena relevant to adolescents’ emotion regulation. First, a baseline was established of relevant phenomena discussed in theory and recent narrative reviews. Then, article keywords and abstracts were analyzed using text mining, examining term frequency as an indicator of relevance and term co-occurrence as an indicator of association. The results reflected themes commonly featured in theory and narrative reviews, such as socialization and neurocognitive development, but also identified undertheorized themes, such as developmental disorders, physical health, external stressors, structural disadvantage, substance use, identity and moral development, and sexual development. The findings illustrate how text mining systematic reviews, a novel approach, may complement narrative reviews. Future theoretical work might integrate these undertheorized themes into an overarching framework, and empirical research might consider them as promising areas for future research, or as potential confounders in research on adolescents’ emotion regulation.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Van Lissa","given":"Caspar J."}],"citation-key":"vanlissaMappingPhenomenaRelevant2022","container-title":"Adolescent Research Review","container-title-short":"Adolescent Res Rev","DOI":"10.1007/s40894-021-00160-7","ISSN":"2363-8346, 2363-8354","issue":"1","issued":{"date-parts":[["2022",3]]},"language":"en","page":"127-139","source":"DOI.org (Crossref)","title":"Mapping Phenomena Relevant to Adolescent Emotion Regulation: A Text-Mining Systematic Review","title-short":"Mapping Phenomena Relevant to Adolescent Emotion Regulation","type":"article-journal","URL":"https://link.springer.com/10.1007/s40894-021-00160-7","volume":"7"},{"id":"wolfHuggingfaceTransformersStateoftheart2019","author":[{"family":"Wolf","given":"Thomas"},{"family":"Debut","given":"Lysandre"},{"family":"Sanh","given":"Victor"},{"family":"Chaumond","given":"Julien"},{"family":"Delangue","given":"Clement"},{"family":"Moi","given":"Anthony"},{"family":"Cistac","given":"Pierric"},{"family":"Rault","given":"Tim"},{"family":"Louf","given":"Rémi"},{"family":"Funtowicz","given":"Morgan"},{"literal":"others"}],"citation-key":"wolfHuggingfaceTransformersStateoftheart2019","container-title":"arXiv preprint arXiv:1910.03771","issued":{"date-parts":[["2019"]]},"title":"Huggingface's transformers: State-of-the-art natural language processing","type":"article-journal"},{"id":"wolfTransformersStateoftheArtNatural2020","abstract":"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered stateof-the art Transformer architectures under a uniﬁed API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/ huggingface/transformers.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Wolf","given":"Thomas"},{"family":"Debut","given":"Lysandre"},{"family":"Sanh","given":"Victor"},{"family":"Chaumond","given":"Julien"},{"family":"Delangue","given":"Clement"},{"family":"Moi","given":"Anthony"},{"family":"Cistac","given":"Pierric"},{"family":"Rault","given":"Tim"},{"family":"Louf","given":"Remi"},{"family":"Funtowicz","given":"Morgan"},{"family":"Davison","given":"Joe"},{"family":"Shleifer","given":"Sam"},{"family":"Von Platen","given":"Patrick"},{"family":"Ma","given":"Clara"},{"family":"Jernite","given":"Yacine"},{"family":"Plu","given":"Julien"},{"family":"Xu","given":"Canwen"},{"family":"Le Scao","given":"Teven"},{"family":"Gugger","given":"Sylvain"},{"family":"Drame","given":"Mariama"},{"family":"Lhoest","given":"Quentin"},{"family":"Rush","given":"Alexander"}],"citation-key":"wolfTransformersStateoftheArtNatural2020","container-title":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations","DOI":"10.18653/v1/2020.emnlp-demos.6","event-place":"Online","event-title":"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations","issued":{"date-parts":[["2020"]]},"language":"en","page":"38-45","publisher":"Association for Computational Linguistics","publisher-place":"Online","source":"DOI.org (Crossref)","title":"Transformers: State-of-the-Art Natural Language Processing","title-short":"Transformers","type":"paper-conference","URL":"https://www.aclweb.org/anthology/2020.emnlp-demos.6"},{"id":"yangSurveyExtractionCausal2022","abstract":"As an essential component of human cognition, cause–effect relations appear frequently in text, and curating cause–effect relations from text helps in building causal networks for predictive tasks. Existing causality extraction techniques include knowledge-based, statistical machine learning (ML)-based, and deep learning-based approaches. Each method has its advantages and weaknesses. For example, knowledge-based methods are understandable but require extensive manual domain knowledge and have poor cross-domain applicability. Statistical machine learning methods are more automated because of natural language processing (NLP) toolkits. However, feature engineering is labor-intensive, and toolkits may lead to error propagation. In the past few years, deep learning techniques attract substantial attention from NLP researchers because of its powerful representation learning ability and the rapid increase in computational resources. Their limitations include high computational costs and a lack of adequate annotated training data. In this paper, we conduct a comprehensive survey of causality extraction. We initially introduce primary forms existing in the causality extraction: explicit intra-sentential causality, implicit causality, and inter-sentential causality. Next, we list benchmark datasets and modeling assessment methods for causal relation extraction. Then, we present a structured overview of the three techniques with their representative systems. Lastly, we highlight existing open challenges with their potential directions.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Yang","given":"Jie"},{"family":"Han","given":"Soyeon Caren"},{"family":"Poon","given":"Josiah"}],"citation-key":"yangSurveyExtractionCausal2022","container-title":"Knowledge and Information Systems","container-title-short":"Knowl Inf Syst","DOI":"10.1007/s10115-022-01665-w","ISSN":"0219-1377, 0219-3116","issue":"5","issued":{"date-parts":[["2022",5]]},"language":"en","page":"1161-1186","source":"DOI.org (Crossref)","title":"A survey on extraction of causal relations from natural language text","type":"article-journal","URL":"https://link.springer.com/10.1007/s10115-022-01665-w","volume":"64"},{"id":"zhaoBiomedicalCrosssentenceRelation2021","abstract":"Most biomedical information extraction efforts are focused on binary relations, there is a strong need to extract drug–gene–mutation n-ary relations among cross-sentences. In recent years, endto-end biomedical relation extraction with sequence-based or dependency-based method has gained increasing attention. However, handling global dependencies and structural information remains challenges for sequence-based and dependency-based models. Joint exploitation of sequence and graph information may improve biomedical cross-sentence relation extraction. In this paper, we present a hybrid model for extracting biomedical relation in a cross-sentence which aims to address these problems. Our models rely on the self-attention mechanism that directly draws the global dependency relation of the sentence. Furthermore, to preserve the dependency structural information between the words that contain the syntactic dependency relations, we employ graph convolutional networks that encode the dependency structural information to guide the multihead attention to learn the dependency relation. Through extensive experiments on benchmark datasets, we demonstrated the effectiveness of our method.","accessed":{"date-parts":[["2024",2,27]]},"author":[{"family":"Zhao","given":"Di"},{"family":"Wang","given":"Jian"},{"family":"Lin","given":"Hongfei"},{"family":"Wang","given":"Xin"},{"family":"Yang","given":"Zhihao"},{"family":"Zhang","given":"Yijia"}],"citation-key":"zhaoBiomedicalCrosssentenceRelation2021","container-title":"Applied Soft Computing","container-title-short":"Applied Soft Computing","DOI":"10.1016/j.asoc.2021.107230","ISSN":"15684946","issued":{"date-parts":[["2021",6]]},"language":"en","page":"107230","source":"DOI.org (Crossref)","title":"Biomedical cross-sentence relation extraction via multihead attention and graph convolutional networks","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S1568494621001538","volume":"104"},{"id":"reiter2017theory","type":"article-journal","note":"Citation Key: reiter2017theory","title":"Theory and methodology of exploratory social science research","author":[{"family":"Reiter","given":"Bernd"}],"issued":{"date-parts":[["2017"]]},"citation-key":"reiter2017theory","library":"My Library","citekey":"reiter2017theory"},{"id":"riaz2013toward","type":"paper-conference","container-title":"Proceedings of the SIGDIAL 2013 conference","note":"Citation Key: riaz2013toward","page":"21–30","title":"Toward a better understanding of causality between verbal events: Extraction and analysis of the causal power of verb-verb associations","author":[{"family":"Riaz","given":"Mehwish"},{"family":"Girju","given":"Roxana"}],"issued":{"date-parts":[["2013"]]},"citation-key":"riaz2013toward","library":"My Library","citekey":"riaz2013toward"},{"id":"devlin2018bert","type":"article-journal","container-title":"arXiv preprint arXiv:1810.04805","note":"Citation Key: devlin2018bert","title":"Bert: Pre-training of deep bidirectional transformers for language understanding","author":[{"family":"Devlin","given":"Jacob"},{"family":"Chang","given":"Ming-Wei"},{"family":"Lee","given":"Kenton"},{"family":"Toutanova","given":"Kristina"}],"issued":{"date-parts":[["2018"]]},"citation-key":"devlin2018bert","library":"My Library","citekey":"devlin2018bert"},{"id":"kenton2019bert","type":"paper-conference","container-title":"Proceedings of naacL-HLT","note":"Citation Key: kenton2019bert","page":"2","title":"Bert: Pre-training of deep bidirectional transformers for language understanding","volume":"1","author":[{"family":"Kenton","given":"Jacob Devlin Ming-Wei Chang"},{"family":"Toutanova","given":"Lee Kristina"}],"issued":{"date-parts":[["2019"]]},"citation-key":"kenton2019bert","library":"My Library","citekey":"kenton2019bert"},{"id":"grosz2020taboo","type":"article-journal","container-title":"Perspectives on Psychological Science","issue":"5","note":"Citation Key: grosz2020taboo\npublisher: Sage Publications Sage CA: Los Angeles, CA","page":"1243–1255","title":"The taboo against explicit causal inference in nonexperimental psychology","volume":"15","author":[{"family":"Grosz","given":"Michael P"},{"family":"Rohrer","given":"Julia M"},{"family":"Thoemmes","given":"Felix"}],"issued":{"date-parts":[["2020"]]},"citation-key":"grosz2020taboo","library":"My Library","citekey":"grosz2020taboo"},{"id":"moghimifar2020domain","type":"article-journal","container-title":"arXiv preprint arXiv:2011.13549","note":"Citation Key: moghimifar2020domain","title":"Domain adaptative causality encoder","author":[{"family":"Moghimifar","given":"Farhad"},{"family":"Haffari","given":"Gholamreza"},{"family":"Baktashmotlagh","given":"Mahsa"}],"issued":{"date-parts":[["2020"]]},"citation-key":"moghimifar2020domain","library":"My Library","citekey":"moghimifar2020domain"}]