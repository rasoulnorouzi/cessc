
@inproceedings{frattiniAutomaticExtractionCauseeffectrelations2020,
	address = {Virtual Event Australia},
	title = {Automatic extraction of cause-effect-relations from requirements artifacts},
	isbn = {978-1-4503-6768-4},
	url = {https://dl.acm.org/doi/10.1145/3324884.3416549},
	doi = {10.1145/3324884.3416549},
	abstract = {Background: The detection and extraction of causality from natural language sentences have shown great potential in various ﬁelds of application. The ﬁeld of requirements engineering is eligible for multiple reasons: (1) requirements artifacts are primarily written in natural language, (2) causal sentences convey essential context about the subject of requirements, and (3) extracted and formalized causality relations are usable for a (semi-)automatic translation into further artifacts, such as test cases.
Objective: We aim at understanding the value of interactive causality extraction based on syntactic criteria for the context of requirements engineering.
Method: We developed a prototype of a system for automatic causality extraction and evaluate it by applying it to a set of publicly available requirements artifacts, determining whether the automatic extraction reduces the manual eﬀort of requirements formalization.
Result: During the evaluation we analyzed 4457 natural language sentences from 18 requirements documents, 558 of which were causal (12.52\%). The best evaluation of a requirements document provided an automatic extraction of 48.57\% cause-eﬀect graphs on average, which demonstrates the feasibility of the approach. Limitation: The feasibility of the approach has been proven in theory but lacks exploration of being scaled up for practical use. Evaluating the applicability of the automatic causality extraction for a requirements engineer is left for future research.
Conclusion: A syntactic approach for causality extraction is viable for the context of requirements engineering and can aid a pipeline towards an automatic generation of further artifacts from requirements artifacts.},
	language = {en},
	urldate = {2023-05-31},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Frattini, Julian and Junker, Maximilian and Unterkalmsteiner, Michael and Mendez, Daniel},
	month = dec,
	year = {2020},
	pages = {561--572},
	file = {Frattini et al. - 2020 - Automatic extraction of cause-effect-relations fro.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\WW2JCKYD\\Frattini et al. - 2020 - Automatic extraction of cause-effect-relations fro.pdf:application/pdf},
}

@article{zhaoBiomedicalCrosssentenceRelation2021,
	title = {Biomedical cross-sentence relation extraction via multihead attention and graph convolutional networks},
	volume = {104},
	issn = {15684946},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494621001538},
	doi = {10.1016/j.asoc.2021.107230},
	abstract = {Most biomedical information extraction efforts are focused on binary relations, there is a strong need to extract drug–gene–mutation n-ary relations among cross-sentences. In recent years, endto-end biomedical relation extraction with sequence-based or dependency-based method has gained increasing attention. However, handling global dependencies and structural information remains challenges for sequence-based and dependency-based models. Joint exploitation of sequence and graph information may improve biomedical cross-sentence relation extraction. In this paper, we present a hybrid model for extracting biomedical relation in a cross-sentence which aims to address these problems. Our models rely on the self-attention mechanism that directly draws the global dependency relation of the sentence. Furthermore, to preserve the dependency structural information between the words that contain the syntactic dependency relations, we employ graph convolutional networks that encode the dependency structural information to guide the multihead attention to learn the dependency relation. Through extensive experiments on benchmark datasets, we demonstrated the effectiveness of our method.},
	language = {en},
	urldate = {2023-05-31},
	journal = {Applied Soft Computing},
	author = {Zhao, Di and Wang, Jian and Lin, Hongfei and Wang, Xin and Yang, Zhihao and Zhang, Yijia},
	month = jun,
	year = {2021},
	pages = {107230},
	file = {Zhao et al. - 2021 - Biomedical cross-sentence relation extraction via .pdf:C\:\\Users\\norouzin\\Zotero\\storage\\GHNU3YL5\\Zhao et al. - 2021 - Biomedical cross-sentence relation extraction via .pdf:application/pdf},
}

@article{hassanzadehCausalKnowledgeExtraction2020,
	title = {Causal {Knowledge} {Extraction} through {Large}-{Scale} {Text} {Mining}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/7092},
	doi = {10.1609/aaai.v34i09.7092},
	abstract = {In this demonstration, we present a system for mining causal knowledge from large corpuses of text documents, such as millions of news articles. Our system provides a collection of APIs for causal analysis and retrieval. These APIs enable searching for the effects of a given cause and the causes of a given effect, as well as the analysis of existence of causal relation given a pair of phrases. The analysis includes a score that indicates the likelihood of the existence of a causal relation. It also provides evidence from an input corpus supporting the existence of a causal relation between input phrases. Our system uses generic unsupervised and weakly supervised methods of causal relation extraction that do not impose semantic constraints on causes and effects. We show example use cases developed for a commercial application in enterprise risk management.},
	language = {en},
	number = {09},
	urldate = {2023-05-31},
	journal = {AAAI},
	author = {Hassanzadeh, Oktie and Bhattacharjya, Debarun and Feblowitz, Mark and Srinivas, Kavitha and Perrone, Michael and Sohrabi, Shirin and Katz, Michael},
	month = apr,
	year = {2020},
	pages = {13610--13611},
	file = {Hassanzadeh et al. - 2020 - Causal Knowledge Extraction through Large-Scale Te.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\EDQI294Q\\Hassanzadeh et al. - 2020 - Causal Knowledge Extraction through Large-Scale Te.pdf:application/pdf},
}

@inproceedings{yangCausalPatternRepresentation2022,
	address = {Sanya China},
	title = {Causal {Pattern} {Representation} {Learning} for {Extracting} {Causality} from {Literature}},
	isbn = {978-1-4503-9906-7},
	url = {https://dl.acm.org/doi/10.1145/3578741.3578787},
	doi = {10.1145/3578741.3578787},
	abstract = {Extracting causality from literature has become an important task due to the essential role of causality. Traditional methods use pattern matching to extract causality, requiring domain knowledge and extensive human effort. Recent researches focus on utilizing pretrained language models due to their success in Natural Language Processing (NLP). However, long sentences in literature hinders the performance of causality extraction. In this paper, we propose to focus on the representation of causal virtual pattern {\textless}head\_entity, causal\_virtual\_trigger, tail\_entity{\textgreater} and design a Causal Pattern Representation Learning (CPRL) method to tackle this challenge. For the causal\_virtual\_trigger representation, CPRL applies the attention mechanism on the shortest dependency path between entities to filter irrelevant information. For the head\_entity and tail\_entity representation, CPRL applies graph convolution networks to encode word dependency on entities. By crawling healthrelated literature abstracts, we create a new causality extraction dataset, namely HealthCE, with a size of 3479. Experiments on HealthCE demonstrate the effectiveness of our approach over existing causality extraction and general relation extraction baselines on the task of causality extraction.},
	language = {en},
	urldate = {2023-05-31},
	booktitle = {Proceedings of the 2022 5th {International} {Conference} on {Machine} {Learning} and {Natural} {Language} {Processing}},
	publisher = {ACM},
	author = {Yang, Jiaoyun and Xiong, Hao and Zhang, Hongjin and Hu, Min and An, Ning},
	month = dec,
	year = {2022},
	pages = {229--233},
	file = {Yang et al. - 2022 - Causal Pattern Representation Learning for Extract.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\Z339674C\\Yang et al. - 2022 - Causal Pattern Representation Learning for Extract.pdf:application/pdf},
}

@article{liCausalityExtractionBased2021,
	title = {Causality extraction based on self-attentive {BiLSTM}-{CRF} with transferred embeddings},
	volume = {423},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231220316027},
	doi = {10.1016/j.neucom.2020.08.078},
	abstract = {Causality extraction from natural language texts is a challenging open problem in artiﬁcial intelligence. Existing methods utilize patterns, constraints, and machine learning techniques to extract causality, heavily depending on domain knowledge and requiring considerable human effort and time for feature engineering. In this paper, we formulate causality extraction as a sequence labeling problem based on a novel causality tagging scheme. On this basis, we propose a neural causality extractor with the BiLSTM-CRF model as the backbone, named SCITE (Self-attentive BiLSTM-CRF wIth Transferred Embeddings), which can directly extract cause and effect without extracting candidate causal pairs and identifying their relations separately. To address the problem of data insufﬁciency, we transfer contextual string embeddings, also known as Flair embeddings, which are trained on a large corpus in our task. In addition, to improve the performance of causality extraction, we introduce a multihead selfattention mechanism into SCITE to learn the dependencies between causal words. We evaluate our method on a public dataset, and experimental results demonstrate that our method achieves signiﬁcant and consistent improvement compared to baselines.},
	language = {en},
	urldate = {2023-05-31},
	journal = {Neurocomputing},
	author = {Li, Zhaoning and Li, Qi and Zou, Xiaotian and Ren, Jiangtao},
	month = jan,
	year = {2021},
	pages = {207--219},
	file = {A_Survey_on_the_Identification_of_Causal_Relation_in_Texts.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\WX42RNIC\\A_Survey_on_the_Identification_of_Causal_Relation_in_Texts.pdf:application/pdf;Li et al. - 2021 - Causality extraction based on self-attentive BiLST.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\R97NY2FA\\Li et al. - 2021 - Causality extraction based on self-attentive BiLST.pdf:application/pdf},
}

@misc{achakulvisutClaimExtractionBiomedical2020,
	title = {Claim {Extraction} in {Biomedical} {Publications} using {Deep} {Discourse} {Model} and {Transfer} {Learning}},
	url = {http://arxiv.org/abs/1907.00962},
	abstract = {Claims are a fundamental unit of scientiﬁc discourse. The exponential growth in the number of scientiﬁc publications makes automatic claim extraction an important problem for researchers who are overwhelmed by this information overload. Such an automated claim extraction system is useful for both manual and programmatic exploration of scientiﬁc knowledge. In this paper, we introduce a new dataset of 1,500 scientiﬁc abstracts from the biomedical domain with expert annotations for each sentence indicating whether the sentence presents a scientiﬁc claim. We introduce a new model for claim extraction and compare it to several baseline models including rule-based and deep learning techniques. Moreover, we show that using a transfer learning approach with a ﬁne-tuning step allows us to improve performance from a large discourse-annotated dataset. Our ﬁnal model increases F1-score by over 14 percent points compared to a baseline model without transfer learning. We release a publicly accessible tool for discourse and claims prediction along with an annotation tool. We discuss further applications beyond biomedical literature.},
	language = {en},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Achakulvisut, Titipat and Bhagavatula, Chandra and Acuna, Daniel and Kording, Konrad},
	month = jan,
	year = {2020},
	note = {arXiv:1907.00962 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Achakulvisut et al. - 2020 - Claim Extraction in Biomedical Publications using .pdf:C\:\\Users\\norouzin\\Zotero\\storage\\222DSJJQ\\Achakulvisut et al. - 2020 - Claim Extraction in Biomedical Publications using .pdf:application/pdf},
}

@article{chenComplexCausalExtraction2022,
	title = {Complex {Causal} {Extraction} of {Fusion} of {Entity} {Location} {Sensing} and {Graph} {Attention} {Networks}},
	volume = {13},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/13/8/364},
	doi = {10.3390/info13080364},
	abstract = {At present, there is no uniform deﬁnition of annotation schemes for causal extraction, and existing methods are limited by the dependence of relations on long spans, which makes complex sentences such as multi-causal relations and nested causal relations difﬁcult to extract. To solve these problems, a head-to-tail entity annotation method is proposed, which can express the complete semantics of complex causal relations and clearly describe the boundaries of entities. Based on this, a causal model, RPA-GCN (relation position and attention-graph convolutional networks), is constructed, incorporating GAT (graph attention network) and entity location perception. The attention layer is combined with a dependency tree to enhance the model’s ability to perceive relational features, and a bi-directional graph convolutional network is constructed to further capture the deep interaction information between entities and relationships. Finally, the classiﬁer iteratively predicts the relationship of each word pair in the sentence and analyzes all causal pairs in the sentence by a scoring function. Experiments on SemEval 2010 task 8 and the Altlex dataset show that our proposed method has signiﬁcant advantages in solving complex causal extraction compared to state-of-the-art methods.},
	language = {en},
	number = {8},
	urldate = {2023-05-31},
	journal = {Information},
	author = {Chen, Yang and Wan, Weibing and Hu, Jimi and Wang, Yuxuan and Huang, Bo},
	month = jul,
	year = {2022},
	pages = {364},
	file = {Chen et al. - 2022 - Complex Causal Extraction of Fusion of Entity Loca.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\XCUXW342\\Chen et al. - 2022 - Complex Causal Extraction of Fusion of Entity Loca.pdf:application/pdf},
}

@misc{yangSurveyExtractionCausal2021,
	title = {A {Survey} on {Extraction} of {Causal} {Relations} from {Natural} {Language} {Text}},
	url = {http://arxiv.org/abs/2101.06426},
	abstract = {As an essential component of human cognition, cause-eﬀect relations appear frequently in text, and curating cause-eﬀect relations from text helps in building causal networks for predictive tasks. Existing causality extraction techniques include knowledge-based, statistical machine learning (ML)-based, and deep learning-based approaches. Each method has its advantages and weaknesses. For example, knowledge-based methods are understandable but require extensive manual domain knowledge and have poor cross-domain applicability. Statistical machine learning methods are more automated because of natural language processing (NLP) toolkits. However, feature engineering is labor-intensive, and toolkits may lead to error propagation. In the past few years, deep learning techniques attract substantial attention from NLP researchers because of its’ powerful representation learning ability and the rapid increase in computational resources. Their limitations include high computational costs and a lack of adequate annotated training data. In this paper, we conduct a comprehensive survey of causality extraction. We initially introduce primary forms existing in the causality extraction: explicit intra-sentential causality, implicit causality, and inter-sentential causality. Next, we list benchmark datasets and modeling assessment methods for causal relation extraction. Then, we present a structured overview of the three techniques with their representative systems. Lastly, we highlight existing open challenges with their potential directions.},
	language = {en},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Yang, Jie and Han, Soyeon Caren and Poon, Josiah},
	month = oct,
	year = {2021},
	note = {arXiv:2101.06426 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Yang et al. - 2021 - A Survey on Extraction of Causal Relations from Na.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\L5BCNKQF\\Yang et al. - 2021 - A Survey on Extraction of Causal Relations from Na.pdf:application/pdf},
}

@article{pengCrossSentenceAryRelation2017,
	title = {Cross-{Sentence} \textit{{N}} -ary {Relation} {Extraction} with {Graph} {LSTMs}},
	volume = {5},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/43389},
	doi = {10.1162/tacl_a_00049},
	abstract = {Past work in relation extraction has focused on binary relations in single sentences. Recent NLP inroads in high-value domains have sparked interest in the more general setting of extracting n-ary relations that span multiple sentences. In this paper, we explore a general relation extraction framework based on graph long short-term memory networks (graph LSTMs) that can be easily extended to cross-sentence n-ary relation extraction. The graph formulation provides a uniﬁed way of exploring different LSTM approaches and incorporating various intra-sentential and intersentential dependencies, such as sequential, syntactic, and discourse relations. A robust contextual representation is learned for the entities, which serves as input to the relation classiﬁer. This simpliﬁes handling of relations with arbitrary arity, and enables multi-task learning with related relations. We evaluate this framework in two important precision medicine settings, demonstrating its effectiveness with both conventional supervised learning and distant supervision. Cross-sentence extraction produced larger knowledge bases. and multi-task learning signiﬁcantly improved extraction accuracy. A thorough analysis of various LSTM approaches yielded useful insight the impact of linguistic analysis on extraction accuracy.},
	language = {en},
	urldate = {2023-05-31},
	journal = {TACL},
	author = {Peng, Nanyun and Poon, Hoifung and Quirk, Chris and Toutanova, Kristina and Yih, Wen-tau},
	month = dec,
	year = {2017},
	pages = {101--115},
	file = {Infant and Child Development - 2022 - Scheel - Why most psychological research findings are not even wrong.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\LYQ3P4EK\\Infant and Child Development - 2022 - Scheel - Why most psychological research findings are not even wrong.pdf:application/pdf;Peng et al. - 2017 - Cross-Sentence N -ary Relation Extraction w.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\UL2D4MN3\\Peng et al. - 2017 - Cross-Sentence N -ary Relation Extraction w.pdf:application/pdf},
}

@article{zhaoDocumentlevelEventCausality2021,
	title = {Document-level event causality identification via graph inference mechanism},
	volume = {561},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S002002552100116X},
	doi = {10.1016/j.ins.2021.01.078},
	abstract = {Event causality identiﬁcation is an important research task in natural language processing. Existing methods largely focus on identifying explicit causal relations, and give poor performance in implicit causalities, especially in the document level. In this paper, we formalize event causality identiﬁcation as a graph-based edge prediction problem and propose a novel document-level context-based graph inference mechanism. Speciﬁcally, we use attention-based neural networks to automatically extract document-level contextual information, and a direction-sensitive graph inference mechanism to achieve information transfer and interaction among event causalities. Experimental results on the EventStoryLine v1.5 dataset show that our approach outperforms previous methods and baseline systems by a large margin in F1-score metrics (2.45\% improvement on intra-sentence causalities and 3.08\% improvement on cross-sentence causalities). Further analysis demonstrates that our model can effectively capture the document-level contextual information and latent causal information among events.},
	language = {en},
	urldate = {2023-05-31},
	journal = {Information Sciences},
	author = {Zhao, Kun and Ji, Donghong and He, Fazhi and Liu, Yijiang and Ren, Yafeng},
	month = jun,
	year = {2021},
	pages = {115--129},
	file = {Zhao et al. - 2021 - Document-level event causality identification via .pdf:C\:\\Users\\norouzin\\Zotero\\storage\\XLVWR95Y\\Zhao et al. - 2021 - Document-level event causality identification via .pdf:application/pdf},
}

@misc{burdissoIDIAPersCausalNews2022,
	title = {{IDIAPers} @ {Causal} {News} {Corpus} 2022: {Efficient} {Causal} {Relation} {Identification} {Through} a {Prompt}-based {Few}-shot {Approach}},
	shorttitle = {{IDIAPers} @ {Causal} {News} {Corpus} 2022},
	url = {http://arxiv.org/abs/2209.03895},
	abstract = {In this paper, we describe our participation in the subtask 1 of CASE-2022, Event Causality Identification with Casual News Corpus. We address the Causal Relation Identification (CRI) task by exploiting a set of simple yet complementary techniques for fine-tuning language models (LMs) on a small number of annotated examples (i.e., a few-shot configuration). We follow a prompt-based prediction approach for fine-tuning LMs in which the CRI task is treated as a masked language modeling problem (MLM). This approach allows LMs natively pre-trained on MLM problems to directly generate textual responses to CRI-specific prompts. We compare the performance of this method against ensemble techniques trained on the entire dataset. Our best-performing submission was fine-tuned with only 256 instances per class, 15.7\% of the all available data, and yet obtained the second-best precision (0.82), third-best accuracy (0.82), and an F1-score (0.85) very close to what was reported by the winner team (0.86).},
	language = {en},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Burdisso, Sergio and Zuluaga-Gomez, Juan and Villatoro-Tello, Esau and Fajcik, Martin and Singh, Muskaan and Smrz, Pavel and Motlicek, Petr},
	month = oct,
	year = {2022},
	note = {arXiv:2209.03895 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Burdisso et al. - 2022 - IDIAPers @ Causal News Corpus 2022 Efficient Caus.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\DPYRFTPE\\Burdisso et al. - 2022 - IDIAPers @ Causal News Corpus 2022 Efficient Caus.pdf:application/pdf},
}

@incollection{jansenExtractingCoreClaims2017,
	address = {Cham},
	title = {Extracting {Core} {Claims} from {Scientific} {Articles}},
	volume = {765},
	isbn = {978-3-319-67467-4 978-3-319-67468-1},
	url = {http://link.springer.com/10.1007/978-3-319-67468-1_3},
	abstract = {The number of scientific articles has grown rapidly over the years and there are no signs that this growth will slow down in the near future. Because of this, it becomes increasingly difficult to keep up with the latest developments in a scientific field. To address this problem, we present here an approach to help researchers learn about the latest developments and findings by extracting in a normalized form core claims from scientific articles. This normalized representation is a controlled natural language of English sentences called AIDA, which has been proposed in previous work as a method to formally structure and organize scientific findings and discourse. We show how such AIDA sentences can be automatically extracted by detecting the core claim of an article, checking for AIDA compliance, and – if necessary – transforming it into a compliant sentence. While our algorithm is still far from perfect, our results indicate that the different steps are feasible and they support the claim that AIDA sentences might be a promising approach to improve scientific communication in the future.},
	language = {en},
	urldate = {2023-05-31},
	booktitle = {{BNAIC} 2016: {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Jansen, Tom and Kuhn, Tobias},
	editor = {Bosse, Tibor and Bredeweg, Bert},
	year = {2017},
	doi = {10.1007/978-3-319-67468-1_3},
	note = {Series Title: Communications in Computer and Information Science},
	pages = {32--46},
	file = {Jansen and Kuhn - 2017 - Extracting Core Claims from Scientific Articles.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\Q8BKHILA\\Jansen and Kuhn - 2017 - Extracting Core Claims from Scientific Articles.pdf:application/pdf},
}

@article{pluvinageExtractingScientificResults,
	title = {Extracting scientific results from research articles},
	language = {en},
	author = {Pluvinage, Lucas},
	file = {Pluvinage - Extracting scientific results from research articl.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\JL8CNRH9\\Pluvinage - Extracting scientific results from research articl.pdf:application/pdf},
}

@inproceedings{tranphuGraphConvolutionalNetworks2021,
	address = {Online},
	title = {Graph {Convolutional} {Networks} for {Event} {Causality} {Identification} with {Rich} {Document}-level {Structures}},
	url = {https://aclanthology.org/2021.naacl-main.273},
	doi = {10.18653/v1/2021.naacl-main.273},
	language = {en},
	urldate = {2023-05-31},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Tran Phu, Minh and Nguyen, Thien Huu},
	year = {2021},
	pages = {3480--3490},
	file = {Tran Phu and Nguyen - 2021 - Graph Convolutional Networks for Event Causality I.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\6JW7EGLK\\Tran Phu and Nguyen - 2021 - Graph Convolutional Networks for Event Causality I.pdf:application/pdf},
}

@book{lauwAdvancesKnowledgeDiscovery2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Advances in {Knowledge} {Discovery} and {Data} {Mining}: 24th {Pacific}-{Asia} {Conference}, {PAKDD} 2020, {Singapore}, {May} 11–14, 2020, {Proceedings}, {Part} {I}},
	volume = {12084},
	isbn = {978-3-030-47425-6 978-3-030-47426-3},
	shorttitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	url = {http://link.springer.com/10.1007/978-3-030-47426-3},
	language = {en},
	urldate = {2023-05-31},
	publisher = {Springer International Publishing},
	editor = {Lauw, Hady W. and Wong, Raymond Chi-Wing and Ntoulas, Alexandros and Lim, Ee-Peng and Ng, See-Kiong and Pan, Sinno Jialin},
	year = {2020},
	doi = {10.1007/978-3-030-47426-3},
	file = {Causal Knowledge Extraction from Scholarly Papers.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\XA6IMFRL\\Causal Knowledge Extraction from Scholarly Papers.pdf:application/pdf;Fried2020_Theory_Preprint.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\E4YIHMAN\\Fried2020_Theory_Preprint.pdf:application/pdf;Lauw et al. - 2020 - Advances in Knowledge Discovery and Data Mining 2.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\8DFL6W6V\\Lauw et al. - 2020 - Advances in Knowledge Discovery and Data Mining 2.pdf:application/pdf},
}

@inproceedings{sahuIntersentenceRelationExtraction2019,
	address = {Florence, Italy},
	title = {Inter-sentence {Relation} {Extraction} with {Document}-level {Graph} {Convolutional} {Neural} {Network}},
	url = {https://www.aclweb.org/anthology/P19-1423},
	doi = {10.18653/v1/P19-1423},
	abstract = {Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-afﬁne pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.},
	language = {en},
	urldate = {2023-05-31},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sahu, Sunil Kumar and Christopoulou, Fenia and Miwa, Makoto and Ananiadou, Sophia},
	year = {2019},
	pages = {4309--4316},
	file = {Sahu et al. - 2019 - Inter-sentence Relation Extraction with Document-l.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\LIYBQ7QM\\Sahu et al. - 2019 - Inter-sentence Relation Extraction with Document-l.pdf:application/pdf},
}

@article{mirzaAnalysisCausalityEvents,
	title = {An {Analysis} of {Causality} between {Events} and its {Relation} to {Temporal} {Information}},
	abstract = {In this work we present an annotation framework to capture causality between events, inspired by TimeML, and a language resource covering both temporal and causal relations. This data set is then used to build an automatic extraction system for causal signals and causal links between given event pairs. The evaluation and analysis of the system’s performance provides an insight into explicit causality in text and the connection between temporal and causal relations.},
	language = {en},
	author = {Mirza, Paramita and Tonelli, Sara},
	file = {Mirza and Tonelli - An Analysis of Causality between Events and its Re.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\782NMRC7\\Mirza and Tonelli - An Analysis of Causality between Events and its Re.pdf:application/pdf},
}

@article{pereraNamedEntityRecognition2020,
	title = {Named {Entity} {Recognition} and {Relation} {Detection} for {Biomedical} {Information} {Extraction}},
	volume = {8},
	issn = {2296-634X},
	url = {https://www.frontiersin.org/article/10.3389/fcell.2020.00673/full},
	doi = {10.3389/fcell.2020.00673},
	abstract = {The number of scientiﬁc publications in the literature is steadily growing, containing our knowledge in the biomedical, health, and clinical sciences. Since there is currently no automatic archiving of the obtained results, much of this information remains buried in textual details not readily available for further usage or analysis. For this reason, natural language processing (NLP) and text mining methods are used for information extraction from such publications. In this paper, we review practices for Named Entity Recognition (NER) and Relation Detection (RD), allowing, e.g., to identify interactions between proteins and drugs or genes and diseases. This information can be integrated into networks to summarize large-scale details on a particular biomedical or clinical problem, which is then amenable for easy data management and further analysis. Furthermore, we survey novel deep learning methods that have recently been introduced for such tasks.},
	language = {en},
	urldate = {2023-05-31},
	journal = {Front. Cell Dev. Biol.},
	author = {Perera, Nadeesha and Dehmer, Matthias and Emmert-Streib, Frank},
	month = aug,
	year = {2020},
	pages = {673},
	file = {Perera et al. - 2020 - Named Entity Recognition and Relation Detection fo.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\95ZRTDAJ\\Perera et al. - 2020 - Named Entity Recognition and Relation Detection fo.pdf:application/pdf},
}

@misc{guptaNeuralRelationExtraction2019,
	title = {Neural {Relation} {Extraction} {Within} and {Across} {Sentence} {Boundaries}},
	url = {http://arxiv.org/abs/1810.05102},
	abstract = {Past work in relation extraction mostly focuses on binary relation between entity pairs within single sentence. Recently, the NLP community has gained interest in relation extraction in entity pairs spanning multiple sentences. In this paper, we propose a novel architecture for this task: inter-sentential dependency-based neural networks (iDepNN). iDepNN models the shortest and augmented dependency paths via recurrent and recursive neural networks to extract relationships within (intra-) and across (inter-) sentence boundaries. Compared to SVM and neural network baselines, iDepNN is more robust to false positives in relationships spanning sentences. We evaluate our models on four datasets from newswire (MUC6) and medical (BioNLP shared task) domains that achieve state-of-the-art performance and show a better balance in precision and recall for inter-sentential relationships. We perform better than 11 teams participating in the BioNLP shared task 2016 and achieve a gain of 5.2\% (0.587 vs 0.558) in F1 over the winning team. We also release the crosssentence annotations for MUC6.},
	language = {en},
	urldate = {2023-05-31},
	publisher = {arXiv},
	author = {Gupta, Pankaj and Rajaram, Subburam and Schütze, Hinrich and Andrassy, Bernt and Runkler, Thomas},
	month = jan,
	year = {2019},
	note = {arXiv:1810.05102 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Causal Knowledge Extraction from Scholarly Papers.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\9ABQV6N7\\Causal Knowledge Extraction from Scholarly Papers.pdf:application/pdf;Gupta et al. - 2019 - Neural Relation Extraction Within and Across Sente.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\J8DTGUAK\\Gupta et al. - 2019 - Neural Relation Extraction Within and Across Sente.pdf:application/pdf},
}

@inproceedings{kyriakakisTransferLearningCausal2019,
	address = {Florence, Italy},
	title = {Transfer {Learning} for {Causal} {Sentence} {Detection}},
	url = {https://www.aclweb.org/anthology/W19-5031},
	doi = {10.18653/v1/W19-5031},
	abstract = {We consider the task of detecting sentences that express causality, as a step towards mining causal relations from texts. To bypass the scarcity of causal instances in relation extraction datasets, we exploit transfer learning, namely ELMO and BERT, using a bidirectional GRU with self-attention (BIGRUATT) as a baseline. We experiment with both generic public relation extraction datasets and a new biomedical causal sentence detection dataset, a subset of which we make publicly available. We ﬁnd that transfer learning helps only in very small datasets. With larger datasets, BIGRUATT reaches a performance plateau, then larger datasets and transfer learning do not help.},
	language = {en},
	urldate = {2023-05-31},
	booktitle = {Proceedings of the 18th {BioNLP} {Workshop} and {Shared} {Task}},
	publisher = {Association for Computational Linguistics},
	author = {Kyriakakis, Manolis and Androutsopoulos, Ion and Saudabayev, Artur and Ginés I Ametllé, Joan},
	year = {2019},
	pages = {292--297},
	file = {Causal Knowledge Extraction from Scholarly Papers.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\8KYZ6SHQ\\Causal Knowledge Extraction from Scholarly Papers.pdf:application/pdf;Kyriakakis et al. - 2019 - Transfer Learning for Causal Sentence Detection.pdf:C\:\\Users\\norouzin\\Zotero\\storage\\2IM5Y4VH\\Kyriakakis et al. - 2019 - Transfer Learning for Causal Sentence Detection.pdf:application/pdf},
}

@misc{nakayamaDoccanoTextAnnotation2018,
	title = {doccano: {Text} {Annotation} {Tool} for {Human}},
	url = {https://github.com/doccano/doccano},
	author = {Nakayama, Hiroki and Kubo, Takahiro and Kamura, Junya and Taniguchi, Yasufumi and Liang, Xu},
	year = {2018},
}

@article{paszkeAutomaticDifferentiationPyTorch2017,
	title = {Automatic differentiation in {PyTorch}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
}

@article{wolfHuggingfaceTransformersStateoftheart2019,
	title = {Huggingface's transformers: {State}-of-the-art natural language processing},
	journal = {arXiv preprint arXiv:1910.03771},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and {others}},
	year = {2019},
}

@misc{huggingfaceTransformersDocumentation2024,
	title = {Transformers {Documentation}},
	url = {https://huggingface.co/docs/transformers/index},
	urldate = {2024-02-23},
	journal = {Transformers},
	author = {{Hugging Face}},
	year = {2024},
}

@inproceedings{beltagySciBERTPretrainedLanguage2019,
	address = {Hong Kong, China},
	title = {{SciBERT}: {A} {Pretrained} {Language} {Model} for {Scientific} {Text}},
	url = {https://aclanthology.org/D19-1371},
	doi = {10.18653/v1/D19-1371},
	abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	month = nov,
	year = {2019},
	pages = {3615--3620},
}

@inproceedings{burdissoIDIAPersCausalNews2022a,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {{IDIAPers} @ {Causal} {News} {Corpus} 2022: {Efficient} {Causal} {Relation} {Identification} {Through} a {Prompt}-based {Few}-shot {Approach}},
	url = {https://aclanthology.org/2022.case-1.9},
	doi = {10.18653/v1/2022.case-1.9},
	abstract = {In this paper, we describe our participation in the subtask 1 of CASE-2022, Event Causality Identification with Casual News Corpus. We address the Causal Relation Identification (CRI) task by exploiting a set of simple yet complementary techniques for fine-tuning language models (LMs) on a few annotated examples (i.e., a few-shot configuration).We follow a prompt-based prediction approach for fine-tuning LMs in which the CRI task is treated as a masked language modeling problem (MLM). This approach allows LMs natively pre-trained on MLM tasks to directly generate textual responses to CRI-specific prompts. We compare the performance of this method against ensemble techniques trained on the entire dataset. Our best-performing submission was fine-tuned with only 256 instances per class, 15.7\% of the all available data, and yet obtained the second-best precision (0.82), third-best accuracy (0.82), and an F1-score (0.85) very close to what was reported by the winner team (0.86).},
	booktitle = {Proceedings of the 5th {Workshop} on {Challenges} and {Applications} of {Automated} {Extraction} of {Socio}-political {Events} from {Text} ({CASE})},
	publisher = {Association for Computational Linguistics},
	author = {Burdisso, Sergio and Zuluaga-gomez, Juan Pablo and Villatoro-tello, Esau and Fajcik, Martin and Singh, Muskaan and Smrz, Pavel and Motlicek, Petr},
	editor = {Hürriyetoğlu, Ali and Tanev, Hristo and Zavarella, Vanni and Yörük, Erdem},
	month = dec,
	year = {2022},
	pages = {61--69},
}

@article{chenCausalKnowledgeExtraction2020,
	title = {Causal {Knowledge} {Extraction} from {Scholarly} {Papers} in {Social} {Sciences}},
	volume = {abs/2006.08904},
	url = {https://api.semanticscholar.org/CorpusID:219708346},
	journal = {ArXiv},
	author = {Chen, Victor Zitian and Montano-Campos, Felipe and Zadrozny, Wlodek},
	year = {2020},
}

@article{dettmersQloraEfficientFinetuning2024,
	title = {Qlora: {Efficient} finetuning of quantized llms},
	volume = {36},
	journal = {Advances in Neural Information Processing Systems},
	author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
	year = {2024},
}

@inproceedings{devlinBERTPretrainingDeep2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@inproceedings{moghimifarDomainAdaptativeCausality2020,
	address = {Virtual Workshop},
	title = {Domain {Adaptative} {Causality} {Encoder}},
	url = {https://aclanthology.org/2020.alta-1.1},
	abstract = {Automated discovery of causal relationships from text is a challenging task. Current approaches which are mainly based on the extraction of low-level relations among individual events are limited by the shortage of publicly available labelled data. Therefore, the resulting models perform poorly when applied to a distributionally different domain for which labelled data did not exist at the time of training. To overcome this limitation, in this paper, we leverage the characteristics of dependency trees and adversarial learning to address the tasks of adaptive causality identification and localisation. The term adaptive is used since the training and test data come from two distributionally different datasets, which to the best of our knowledge, this work is the first to address. Moreover, we present a new causality dataset, namely MedCaus, which integrates all types of causality in the text. Our experiments on four different benchmark causality datasets demonstrate the superiority of our approach over the existing baselines, by up to 7\% improvement, on the tasks of identification and localisation of the causal relations from the text.},
	booktitle = {Proceedings of the {The} 18th {Annual} {Workshop} of the {Australasian} {Language} {Technology} {Association}},
	publisher = {Australasian Language Technology Association},
	author = {Moghimifar, Farhad and Haffari, Gholamreza and Baktashmotlagh, Mahsa},
	editor = {Kim, Maria and Beck, Daniel and Mistica, Meladel},
	month = dec,
	year = {2020},
	pages = {1--10},
}

@inproceedings{tanEventCausalityIdentification2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Event {Causality} {Identification} with {Causal} {News} {Corpus} - {Shared} {Task} 3, {CASE} 2022},
	url = {https://aclanthology.org/2022.case-1.28},
	doi = {10.18653/v1/2022.case-1.28},
	abstract = {The Event Causality Identification Shared Task of CASE 2022 involved two subtasks working on the Causal News Corpus. Subtask 1 required participants to predict if a sentence contains a causal relation or not. This is a supervised binary classification task. Subtask 2 required participants to identify the Cause, Effect and Signal spans per causal sentence. This could be seen as a supervised sequence labeling task. For both subtasks, participants uploaded their predictions for a held-out test set, and ranking was done based on binary F1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes the work of the 17 teams that submitted their results to our competition and 12 system description papers that were received. The best F1 scores achieved for Subtask 1 and 2 were 86.19\% and 54.15\%, respectively. All the top-performing approaches involved pre-trained language models fine-tuned to the targeted task. We further discuss these approaches and analyze errors across participants' systems in this paper.},
	booktitle = {Proceedings of the 5th {Workshop} on {Challenges} and {Applications} of {Automated} {Extraction} of {Socio}-political {Events} from {Text} ({CASE})},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Fiona Anting and Hettiarachchi, Hansi and Hürriyetoğlu, Ali and Caselli, Tommaso and Uca, Onur and Liza, Farhana Ferdousi and Oostdijk, Nelleke},
	editor = {Hürriyetoğlu, Ali and Tanev, Hristo and Zavarella, Vanni and Yörük, Erdem},
	month = dec,
	year = {2022},
	pages = {195--208},
}
