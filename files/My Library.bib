@inproceedings{beltagySciBERTPretrainedLanguage2019,
  title = {{{SciBERT}}: {{A Pretrained Language Model}} for {{Scientific Text}}},
  shorttitle = {{{SciBERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  date = {2019},
  pages = {3613--3618},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1371},
  url = {https://www.aclweb.org/anthology/D19-1371},
  urldate = {2024-02-27},
  abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of highquality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github. com/allenai/scibert/.},
  eventtitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\5YFU5F3M\Beltagy et al. - 2019 - SciBERT A Pretrained Language Model for Scientifi.pdf}
}

@article{berkmanUsefulGoodTheory,
  title = {So {{Useful}} as a {{Good Theory}}? {{The Practicality Crisis}} in ({{Social}}) {{Psychological Theory}}},
  author = {Berkman, Elliot T and Wilson, Sylas M},
  abstract = {Practicality was a valued attribute of academic psychological theory during its initial decades, but usefulness has since faded in importance to the field. Theories are now evaluated mainly on their ability to account for decontextualized laboratory data and not their ability to help solve societal problems. With laudable exceptions in the clinical, intergroup, and health domains, most psychological theories have little relevance to people’s everyday lives, poor accessibility to policymakers, or even applicability to the work of other academics who are better positioned to translate the theories to the practical realm. We refer to the lack of relevance, accessibility, and applicability of psychological theory to the rest of society as the practicality crisis. The practicality crisis harms the field in its ability to attract the next generation of scholars and maintain viability at the national level. We describe practical theory and illustrate its use in the field of self-regulation. Psychological theory is historically and scientifically well positioned to become useful should scholars in the field decide to value practicality. We offer a set of incentives to encourage the return of social psychology to the Lewinian vision of a useful science that speaks to pressing social issues.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\228RVCLT\Berkman and Wilson - So Useful as a Good Theory The Practicality Crisi.pdf}
}

@inproceedings{burdissoIDIAPersCausalNews2022,
  title = {{{IDIAPers}} @ {{Causal News Corpus}} 2022: {{Efficient Causal Relation Identification Through}} a {{Prompt-based Few-shot Approach}}},
  shorttitle = {{{IDIAPers}} @ {{Causal News Corpus}} 2022},
  booktitle = {Proceedings of the 5th {{Workshop}} on {{Challenges}} and {{Applications}} of {{Automated Extraction}} of {{Socio-political Events}} from {{Text}} ({{CASE}})},
  author = {Burdisso, Sergio and Zuluaga-gomez, Juan Pablo and Villatoro-tello, Esau and Fajcik, Martin and Singh, Muskaan and Smrz, Pavel and Motlicek, Petr},
  date = {2022},
  pages = {61--69},
  publisher = {{Association for Computational Linguistics}},
  location = {{Abu Dhabi, United Arab Emirates (Hybrid)}},
  doi = {10.18653/v1/2022.case-1.9},
  url = {https://aclanthology.org/2022.case-1.9},
  urldate = {2024-02-27},
  eventtitle = {Proceedings of the 5th {{Workshop}} on {{Challenges}} and {{Applications}} of {{Automated Extraction}} of {{Socio-political Events}} from {{Text}} ({{CASE}})},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\NN6ZTNVH\Burdisso et al. - 2022 - IDIAPers @ Causal News Corpus 2022 Efficient Caus.pdf}
}

@article{busettiCausalityGoodPractice2023,
  title = {Causality Is Good for Practice: Policy Design and Reverse Engineering},
  shorttitle = {Causality Is Good for Practice},
  author = {Busetti, Simone},
  date = {2023-06},
  journaltitle = {Policy Sciences},
  shortjournal = {Policy Sci},
  volume = {56},
  number = {2},
  pages = {419--438},
  issn = {0032-2687, 1573-0891},
  doi = {10.1007/s11077-023-09493-7},
  url = {https://link.springer.com/10.1007/s11077-023-09493-7},
  urldate = {2024-02-27},
  abstract = {Relevance to practice is an open issue for scholars in public policy and public administration. One major problem is the need to produce knowledge that can guide practitioners designing and implementing public interventions in specific contexts. This article claims that investigating the causal mechanisms of policy programs—i.e., modeling why and how they produce outcomes—can contribute to such knowledge. In this regard, mechanisms offer essential information to guide practitioners when replicating, adjusting, and designing interventions. Unfortunately, not all models of mechanisms can inform practice. The article proposes a strategy for design research and practice inspired by reverse engineering: selecting successful programs, causal modeling, assessing the target context, and designing. Scholars should model mechanisms by identifying the program and non-program elements that contribute to the outcome of interest and abstracting their causal powers. Practitioners can use these models, diagnose their target context, and adjust designs to deal with context-specific problems. The proposed research agenda may enhance orientation to practice and offer a middle ground between the search for abstract, general relationships, and single-case analyses.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\JIBKLWS8\Busetti - 2023 - Causality is good for practice policy design and .pdf}
}

@inproceedings{chen2021better,
  title = {Better Few-Shot Text Classification with Pre-Trained Language Model},
  booktitle = {Artificial Neural Networks and Machine {{Learning}}–{{ICANN}} 2021: 30th International Conference on Artificial Neural Networks, Bratislava, Slovakia, September 14–17, 2021, Proceedings, Part {{II}} 30},
  author = {Chen, Zheng and Zhang, Yunchen},
  date = {2021},
  pages = {537--548},
  publisher = {{Springer}}
}

@article{devlinBERTPretrainingDeep,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\AE4KHELD\Devlin et al. - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@article{dingParameterefficientFinetuningLargescale2023,
  title = {Parameter-Efficient Fine-Tuning of Large-Scale Pre-Trained Language Models},
  author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and Yi, Jing and Zhao, Weilin and Wang, Xiaozhi and Liu, Zhiyuan and Zheng, Hai-Tao and Chen, Jianfei and Liu, Yang and Tang, Jie and Li, Juanzi and Sun, Maosong},
  date = {2023-03-02},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {5},
  number = {3},
  pages = {220--235},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00626-4},
  url = {https://www.nature.com/articles/s42256-023-00626-4},
  urldate = {2024-02-27},
  abstract = {Abstract             With the prevalence of pre-trained language models (PLMs) and the pre-training–fine-tuning paradigm, it has been continuously shown that larger models tend to yield better performance. However, as PLMs scale up, fine-tuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, which optimizes a small portion of the model parameters while keeping the rest fixed, drastically cutting down computation and storage costs. In general, it demonstrates that large-scale models could be effectively stimulated by the optimization of a few parameters. Despite the various designs, here we discuss and analyse the approaches under a more consistent and accessible term ‘delta-tuning’, where ‘delta’ a mathematical notation often used to denote changes, is borrowed to refer to the portion of parameters that are ‘changed’ during training. We formally describe the problem and propose a unified categorization criterion for existing delta-tuning methods to explore their correlations and differences. We also discuss the theoretical principles underlying the effectiveness of delta-tuning and interpret them from the perspectives of optimization and optimal control. Furthermore, we provide a holistic empirical study on over 100 natural language processing tasks and investigate various aspects of delta-tuning. With comprehensive study and analysis, our research demonstrates the theoretical and practical properties of delta-tuning in the adaptation of PLMs.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\SP7828AX\Ding et al. - 2023 - Parameter-efficient fine-tuning of large-scale pre.pdf}
}

@misc{doccano,
  title = {{{doccano}}: {{Text}} Annotation Tool for Human},
  author = {Nakayama, Hiroki and Kubo, Takahiro and Kamura, Junya and Taniguchi, Yasufumi and Liang, Xu},
  date = {2018},
  url = {https://github.com/doccano/doccano},
  note = {Software available from https://github.com/doccano/doccano}
}

@article{fleissMeasuringNominalScale1971,
  title = {Measuring Nominal Scale Agreement among Many Raters.},
  author = {Fleiss, Joseph L.},
  date = {1971-11},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  volume = {76},
  number = {5},
  pages = {378--382},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/h0031619},
  url = {https://doi.apa.org/doi/10.1037/h0031619},
  urldate = {2024-02-27},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\BE9VSNPR\Fleiss - 1971 - Measuring nominal scale agreement among many rater.pdf}
}

@inproceedings{frattiniAutomaticExtractionCauseeffectrelations2020,
  title = {Automatic Extraction of Cause-Effect-Relations from Requirements Artifacts},
  booktitle = {Proceedings of the 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  author = {Frattini, Julian and Junker, Maximilian and Unterkalmsteiner, Michael and Mendez, Daniel},
  date = {2020-12-21},
  pages = {561--572},
  publisher = {{ACM}},
  location = {{Virtual Event Australia}},
  doi = {10.1145/3324884.3416549},
  url = {https://dl.acm.org/doi/10.1145/3324884.3416549},
  urldate = {2024-02-27},
  abstract = {Background: The detection and extraction of causality from natural language sentences have shown great potential in various fields of application. The field of requirements engineering is eligible for multiple reasons: (1) requirements artifacts are primarily written in natural language, (2) causal sentences convey essential context about the subject of requirements, and (3) extracted and formalized causality relations are usable for a (semi-)automatic translation into further artifacts, such as test cases. Objective: We aim at understanding the value of interactive causality extraction based on syntactic criteria for the context of requirements engineering. Method: We developed a prototype of a system for automatic causality extraction and evaluate it by applying it to a set of publicly available requirements artifacts, determining whether the automatic extraction reduces the manual effort of requirements formalization. Result: During the evaluation we analyzed 4457 natural language sentences from 18 requirements documents, 558 of which were causal (12.52\%). The best evaluation of a requirements document provided an automatic extraction of 48.57\% cause-effect graphs on average, which demonstrates the feasibility of the approach. Limitation: The feasibility of the approach has been proven in theory but lacks exploration of being scaled up for practical use. Evaluating the applicability of the automatic causality extraction for a requirements engineer is left for future research. Conclusion: A syntactic approach for causality extraction is viable for the context of requirements engineering and can aid a pipeline towards an automatic generation of further artifacts from requirements artifacts.},
  eventtitle = {{{ASE}} '20: 35th {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  isbn = {978-1-4503-6768-4},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\QWW25T47\Frattini et al. - 2020 - Automatic extraction of cause-effect-relations fro.pdf}
}

@inproceedings{gaoMakingPretrainedLanguage2021,
  title = {Making {{Pre-trained Language Models Better Few-shot Learners}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  date = {2021},
  pages = {3816--3830},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-long.295},
  url = {https://aclanthology.org/2021.acl-long.295},
  urldate = {2024-02-27},
  eventtitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\C6M9JI7P\Gao et al. - 2021 - Making Pre-trained Language Models Better Few-shot.pdf}
}

@article{grossStructureCausalChains2018,
  title = {The {{Structure}} of {{Causal Chains}}},
  author = {Gross, Neil},
  date = {2018-12},
  journaltitle = {Sociological Theory},
  shortjournal = {Sociological Theory},
  volume = {36},
  number = {4},
  pages = {343--367},
  issn = {0735-2751, 1467-9558},
  doi = {10.1177/0735275118811377},
  url = {http://journals.sagepub.com/doi/10.1177/0735275118811377},
  urldate = {2024-02-27},
  abstract = {Sociologists are increasingly attentive to the mechanisms responsible for cause-and-effect relationships in the social world. But an aspect of mechanistic causality has not been sufficiently considered. It is well recognized that most phenomena of interest to social science result from multiple mechanisms operating in sequence. However, causal chains—sequentially linked mechanisms and their enabling background conditions—vary not just substantively, by the kind of causal work they do, but also structurally, by their formal properties. In this article, the author examines the nature of causal chains, identifies major structural dimensions along which they differ, and makes a case that a mechanism-based explanation would be enhanced if causal chains and their structures were brought to the analytical forefront.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\GF9GQUUJ\Gross - 2018 - The Structure of Causal Chains.pdf}
}

@article{groszTabooExplicitCausal,
  title = {The {{Taboo Against Explicit Causal Inference}} in {{Nonexperimental Psychology}}},
  author = {Grosz, Michael P and Rohrer, Julia M and Thoemmes, Felix},
  abstract = {Causal inference is a central goal of research. However, most psychologists refrain from explicitly addressing causal research questions and avoid drawing causal inference on the basis of nonexperimental evidence. We argue that this taboo against causal inference in nonexperimental psychology impairs study design and data analysis, holds back cumulative research, leads to a disconnect between original findings and how they are interpreted in subsequent work, and limits the relevance of nonexperimental psychology for policymaking. At the same time, the taboo does not prevent researchers from interpreting findings as causal effects—the inference is simply made implicitly, and assumptions remain unarticulated. Thus, we recommend that nonexperimental psychologists begin to talk openly about causal assumptions and causal effects. Only then can researchers take advantage of recent methodological advances in causal reasoning and analysis and develop a solid understanding of the underlying causal mechanisms that can inform future research, theory, and policymakers.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\SC7LPUE2\Grosz et al. - The Taboo Against Explicit Causal Inference in Non.pdf}
}

@article{hassanzadehCausalKnowledgeExtraction2020,
  title = {Causal {{Knowledge Extraction}} through {{Large-Scale Text Mining}}},
  author = {Hassanzadeh, Oktie and Bhattacharjya, Debarun and Feblowitz, Mark and Srinivas, Kavitha and Perrone, Michael and Sohrabi, Shirin and Katz, Michael},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {09},
  pages = {13610--13611},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i09.7092},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/7092},
  urldate = {2024-02-27},
  abstract = {In this demonstration, we present a system for mining causal knowledge from large corpuses of text documents, such as millions of news articles. Our system provides a collection of APIs for causal analysis and retrieval. These APIs enable searching for the effects of a given cause and the causes of a given effect, as well as the analysis of existence of causal relation given a pair of phrases. The analysis includes a score that indicates the likelihood of the existence of a causal relation. It also provides evidence from an input corpus supporting the existence of a causal relation between input phrases. Our system uses generic unsupervised and weakly supervised methods of causal relation extraction that do not impose semantic constraints on causes and effects. We show example use cases developed for a commercial application in enterprise risk management.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\RCHYK7BU\Hassanzadeh et al. - 2020 - Causal Knowledge Extraction through Large-Scale Te.pdf}
}

@article{hedstromCausalMechanismsSocial2010,
  title = {Causal {{Mechanisms}} in the {{Social Sciences}}},
  author = {Hedström, Peter and Ylikoski, Petri},
  date = {2010-06-01},
  journaltitle = {Annual Review of Sociology},
  shortjournal = {Annu. Rev. Sociol.},
  volume = {36},
  number = {1},
  pages = {49--67},
  issn = {0360-0572, 1545-2115},
  doi = {10.1146/annurev.soc.012809.102632},
  url = {https://www.annualreviews.org/doi/10.1146/annurev.soc.012809.102632},
  urldate = {2024-02-27},
  abstract = {During the past decade, social mechanisms and mechanism-based explanations have received considerable attention in the social sciences as well as in the philosophy of science. This article critically reviews the most important philosophical and social science contributions to the mechanism approach. The first part discusses the idea of mechanismbased explanation from the point of view of philosophy of science and relates it to causation and to the covering-law account of explanation. The second part focuses on how the idea of mechanisms has been used in the social sciences. The final part discusses recent developments in analytical sociology, covering the nature of sociological explananda, the role of theory of action in mechanism-based explanations, Merton’s idea of middle-range theory, and the role of agent-based simulations in the development of mechanism-based explanations.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\39H4XG9V\Hedström and Ylikoski - 2010 - Causal Mechanisms in the Social Sciences.pdf}
}

@online{huggingfaceTransformersDocumentation2024,
  title = {Transformers {{Documentation}}},
  author = {{Hugging Face}},
  date = {2024},
  url = {https://huggingface.co/docs/transformers/index},
  urldate = {2024-02-23},
  organization = {{Transformers}},
  note = {Accessed: 2024-02-23}
}

@online{jiangMistral7B2023,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and family=Casas, given=Diego, prefix=de las, useprefix=false and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
  date = {2023-10-10},
  eprint = {2310.06825},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.06825},
  urldate = {2024-02-27},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Models and code are available at https://mistral.ai/news/announcing-mistral-7b/},
  file = {C:\Users\norouzin\Zotero\storage\6JAI48PJ\Jiang et al. - 2023 - Mistral 7B.pdf}
}

@article{landisMeasurementObserverAgreement1977,
  title = {The {{Measurement}} of {{Observer Agreement}} for {{Categorical Data}}},
  author = {Landis, J. Richard and Koch, Gary G.},
  date = {1977-03},
  journaltitle = {Biometrics},
  shortjournal = {Biometrics},
  volume = {33},
  number = {1},
  eprint = {2529310},
  eprinttype = {jstor},
  pages = {159},
  issn = {0006341X},
  doi = {10.2307/2529310},
  url = {https://www.jstor.org/stable/2529310?origin=crossref},
  urldate = {2024-02-27},
  abstract = {This paper presents a general statistical methodology for the analysis of multivariate categorical data arising from observer reliability studies. The procedure essentially involves the construction of functions of the observed proportions which are directed at the extent to which the observers agree among themselves and the construction of test statistics for hypotheses involving these functions. Tests for interobserver bias are presented in terms of first-order marginal homogeneity and measures of interobserver agreement are developed as generalized kappa-type statistics. These procedures are illustrated with a clinical diagnosis example from the epidemiological literature.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\M8FRWZEP\Landis and Koch - 1977 - The Measurement of Observer Agreement for Categori.pdf}
}

@online{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  date = {2019-07-26},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1907.11692},
  urldate = {2024-02-27},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C:\Users\norouzin\Zotero\storage\PL6TETWQ\Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf}
}

@inproceedings{lopez2009grobid,
  title = {{{GROBID}}: {{Combining}} Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications},
  booktitle = {Research and Advanced Technology for Digital Libraries: 13th European Conference, {{ECDL}} 2009, Corfu, Greece, September 27-{{October}} 2, 2009. {{Proceedings}} 13},
  author = {Lopez, Patrice},
  date = {2009},
  pages = {473--474},
  publisher = {{Springer}}
}

@article{maWhatHowExtracting2023,
  title = {From “What” to “How”: {{Extracting}} the {{Procedural Scientific Information Toward}} the {{Metric-optimization}} in {{AI}}},
  shorttitle = {From “What” to “How”},
  author = {Ma, Yongqiang and Liu, Jiawei and Lu, Wei and Cheng, Qikai},
  date = {2023-05},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {60},
  number = {3},
  pages = {103315},
  issn = {03064573},
  doi = {10.1016/j.ipm.2023.103315},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0306457323000523},
  urldate = {2024-02-27},
  abstract = {In response to the exponential growth of the volume of scientific publications, researchers have proposed a multitude of information extraction methods for extracting entities and relations, such as task, dataset, metric, and method entities. However, the existing methods cannot directly provide readers with procedural scientific information that demonstrates the path to the prob­ lem’s solution. From the perspective of applied science, we propose a novel schema for the applied AI community, namely a metric-driven mechanism schema (Operation, Effect, Direc­ tion). Our schema depicts the procedural scientific information concerning “How to optimize the quantitative metrics for a specific task?” In this paper, we choose papers in the domain of NLP for our study, which is a representative branch of Artificial Intelligence (AI). Specifically, we first construct a dataset that covers the metric-driven mechanisms in single and multiple sentences. Then we propose a framework for extracting metric-driven mechanisms, which includes three sub-models: 1) a mechanism detection model, 2) a query-guided seq2seq mechanism extraction model, and 3) a task recognition model. Finally, a metric-driven mechanism knowledge graph, named MKGNLP, is constructed. Our MKGNLP has over 43K n-ary mechanism relations in the form of (Operation, Effect, Direction, Task). The human evaluation shows that the extracted metricdriven mechanisms in MKGNLP achieve 81.4\% accuracy. Our model also shows the potential for creating applications to assist applied AI scientists to solve specific problems.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\WN3H8LJZ\Ma et al. - 2023 - From “what” to “how” Extracting the Procedural Sc.pdf}
}

@article{meehlAppraisingAmendingTheories1990,
  title = {Appraising and {{Amending Theories}}: {{The Strategy}} of {{Lakatosian Defense}} and {{Two Principles}} That {{Warrant It}}},
  shorttitle = {Appraising and {{Amending Theories}}},
  author = {Meehl, Paul E.},
  date = {1990-04},
  journaltitle = {Psychological Inquiry},
  shortjournal = {Psychological Inquiry},
  volume = {1},
  number = {2},
  pages = {108--141},
  issn = {1047-840X, 1532-7965},
  doi = {10.1207/s15327965pli0102_1},
  url = {http://www.tandfonline.com/doi/abs/10.1207/s15327965pli0102_1},
  urldate = {2024-02-27},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\ZCBJY434\Meehl - 1990 - Appraising and Amending Theories The Strategy of .pdf}
}

@article{moghimifarDomainAdaptativeCausality,
  title = {Domain {{Adaptative Causality Encoder}}},
  author = {Moghimifar, Farhad and Haffari, Gholamreza and Baktashmotlagh, Mahsa},
  abstract = {Automated discovery of causal relationships from text is a challenging task. Current approaches which are mainly based on the extraction of low-level relations among individual events are limited by the shortage of publicly available labelled data. Therefore, the resulting models perform poorly when applied to a distributionally different domain for which labelled data did not exist at the time of training. To overcome this limitation, in this paper, we leverage the characteristics of dependency trees and adversarial learning to address the tasks of adaptive causality identification and localisation. The term adaptive is used since the training and test data come from two distributionally different datasets, which to the best of our knowledge, this work is the first to address. Moreover, we present a new causality dataset, namely MEDCAUS1, which integrates all types of causality in the text. Our experiments on four different benchmark causality datasets demonstrate the superiority of our approach over the existing baselines, by up to 7\% improvement, on the tasks of identification and localisation of the causal relations from the text.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\CIXBNNCS\Moghimifar et al. - Domain Adaptative Causality Encoder.pdf}
}

@article{morley2014academic,
  title = {Academic Phrasebank},
  author = {Morley, John},
  date = {2014},
  journaltitle = {Manchester: University of Manchester}
}

@article{morrisonMethodsUnderstandingsExpressions2020,
  title = {Methods, Understandings, and Expressions of Causality in Educational Research},
  author = {Morrison, Keith and Van Der Werf, Greetje},
  date = {2020-11-16},
  journaltitle = {Educational Research and Evaluation},
  shortjournal = {Educational Research and Evaluation},
  volume = {26},
  number = {7-8},
  pages = {339--343},
  issn = {1380-3611, 1744-4187},
  doi = {10.1080/13803611.2021.1991643},
  url = {https://www.tandfonline.com/doi/full/10.1080/13803611.2021.1991643},
  urldate = {2024-02-27},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\NZPV7F8S\Morrison and Van Der Werf - 2020 - Methods, understandings, and expressions of causal.pdf}
}

@misc{nakayamaDoccanoTextAnnotation2018,
  title = {Doccano: {{Text Annotation Tool}} for {{Human}}},
  author = {Nakayama, Hiroki and Kubo, Takahiro and Kamura, Junya and Taniguchi, Yasufumi and Liang, Xu},
  date = {2018},
  url = {https://github.com/doccano/doccano},
  note = {Software available from https://github.com/doccano/doccano}
}

@article{ottHedgingWeaselWords2018,
  title = {Hedging, {{Weasel Words}}, and {{Truthiness}} in {{Scientific Writing}}},
  author = {Ott, Douglas E.},
  date = {2018},
  journaltitle = {JSLS : Journal of the Society of Laparoendoscopic Surgeons},
  shortjournal = {JSLS},
  volume = {22},
  number = {4},
  pages = {e2018.00063},
  issn = {1086-8089, 1938-3797},
  doi = {10.4293/JSLS.2018.00063},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6311890/},
  urldate = {2024-02-27},
  abstract = {Background and Objectives: Words in scientific discourse must be truthful. Introducing ambiguity or creating a false narrative by insinuating close counts or almost statements as facts that appeal to a truth the writer wants to exist doesn’t make it true. A reader’s personal interpretation because of hedging or weasel words creates an opportunity for truthiness as a belief to become a fact when it isn’t.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\U7WGK79Q\Ott - 2018 - Hedging, Weasel Words, and Truthiness in Scientifi.pdf}
}

@article{paszke2017automatic,
  title = {Automatic Differentiation in Pytorch},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  date = {2017}
}

@article{paszkeAutomaticDifferentiationPyTorch2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  date = {2017}
}

@article{pearl2009causal,
  title = {Causal Inference in Statistics: {{An}} Overview},
  author = {Pearl, Judea},
  date = {2009}
}

@article{reiterTheoryMethodologyExploratory,
  title = {Theory and {{Methodology}} of {{Exploratory Social Science Research}}},
  author = {Reiter, Bernd},
  volume = {5},
  abstract = {Confirmatory, deductive research cannot produce absolute truths, according Karl POPPER (2002). If we accept this premise, then it is worth giving inductive and explorative research another chance. Exploration can produce valid and insightful findings in the social sciences, if conducted in a transparent and self-reflexive way. It can also profit from applying dialectical thinking. This article proposes a rationale for exploration in the social sciences and it elaborates the criteria on which such research must stand.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\B37U3DRK\Reiter - Theory and Methodology of Exploratory Social Scien.pdf}
}

@article{riazBetterUnderstandingCausality,
  title = {Toward a {{Better Understanding}} of {{Causality}} between {{Verbal Events}}: {{Extraction}} and {{Analysis}} of the {{Causal Power}} of {{Verb-Verb Associations}}},
  author = {Riaz, Mehwish and Girju, Roxana},
  abstract = {The identification of causal relations between verbal events is important for achieving natural language understanding. However, the problem has proven notoriously difficult since it is not clear which types of knowledge are necessary to solve this challenging problem close to human level performance. Instead of employing a large set of features proved useful in other NLP tasks, we split the problem in smaller sub problems. Since verbs play a very important role in causal relations, in this paper we harness, explore, and evaluate the predictive power of causal associations of verb-verb pairs. More specifically, we propose a set of knowledge-rich metrics to learn the likelihood of causal relations between verbs. Employing these metrics, we automatically generate a knowledge base (KBc) which identifies three categories of verb pairs: Strongly Causal, Ambiguous, and Strongly Non-causal. The knowledge base is evaluated empirically. The results show that our metrics perform significantly better than the state-of-the-art on the task of detecting causal verbal events.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\W2NUAU2Z\Riaz and Girju - Toward a Better Understanding of Causality between.pdf}
}

@article{sanbonmatsuImpactComplexityMethods2021,
  title = {The {{Impact}} of {{Complexity}} on {{Methods}} and {{Findings}} in {{Psychological Science}}},
  author = {Sanbonmatsu, David M. and Cooley, Emily H. and Butner, Jonathan E.},
  date = {2021-01-21},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {11},
  pages = {580111},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2020.580111},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2020.580111/full},
  urldate = {2024-02-27},
  abstract = {The study of human behavior is severely hampered by logistical problems, ethical and legal constraints, and funding shortfalls. However, the biggest difficulty of conducting social and behavioral research is the extraordinary complexity of the study phenomena. In this article, we review the impact of complexity on research design, hypothesis testing, measurement, data analyses, reproducibility, and the communication of findings in psychological science. The systematic investigation of the world often requires different approaches because of the variability in complexity. Confirmatory testing, multi-factorial designs, survey methods, large samples, and modeling are frequently needed to study complex social and behavioral topics. Complexity impedes the measurement of general constructs, the reproducibility of results and scientific reporting, and the general rigor of research. Many of the benchmarks established by classic work in physical science are not attainable in studies of more complex phenomena. Consequently, the standards used to evaluate scientific research should be tethered to the complexity of the study topic.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\CDPSW73K\Sanbonmatsu et al. - 2021 - The Impact of Complexity on Methods and Findings i.pdf}
}

@article{spadaroCooperationDatabankMachineReadable,
  title = {The {{Cooperation Databank}}: {{Machine-Readable Science Accelerates Research Synthesis}}},
  author = {Spadaro, Giuliana and Tiddi, Ilaria and Columbus, Simon and Jin, Shuxian},
  journaltitle = {Perspectives on Psychological Science},
  abstract = {Publishing studies using standardized, machine-readable formats will enable machines to perform meta-analyses on demand. To build a semantically enhanced technology that embodies these functions, we developed the Cooperation Databank (CoDa)—a databank that contains 2,636 studies on human cooperation (1958–2017) conducted in 78 societies involving 356,283 participants. Experts annotated these studies along 312 variables, including the quantitative results (13,959 effects). We designed an ontology that defines and relates concepts in cooperation research and that can represent the relationships between results of correlational and experimental studies. We have created a research platform that, given the data set, enables users to retrieve studies that test the relation of variables with cooperation, visualize these study results, and perform (a) meta-analyses, (b) metaregressions, (c) estimates of publication bias, and (d) statistical power analyses for future studies. We leveraged the data set with visualization tools that allow users to explore the ontology of concepts in cooperation research and to plot a citation network of the history of studies. CoDa offers a vision of how publishing studies in a machine-readable format can establish institutions and tools that improve scientific practices and knowledge.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\6YQT44N7\Spadaro et al. - The Cooperation Databank Machine-Readable Science.pdf}
}

@inproceedings{tanEventCausalityIdentification2022,
  title = {Event {{Causality Identification}} with {{Causal News Corpus}} - {{Shared Task}} 3, {{CASE}} 2022},
  booktitle = {Proceedings of the 5th {{Workshop}} on {{Challenges}} and {{Applications}} of {{Automated Extraction}} of {{Socio-political Events}} from {{Text}} ({{CASE}})},
  author = {Tan, Fiona Anting and Hettiarachchi, Hansi and Hürriyetoğlu, Ali and Caselli, Tommaso and Uca, Onur and Liza, Farhana Ferdousi and Oostdijk, Nelleke},
  date = {2022},
  pages = {195--208},
  publisher = {{Association for Computational Linguistics}},
  location = {{Abu Dhabi, United Arab Emirates (Hybrid)}},
  doi = {10.18653/v1/2022.case-1.28},
  url = {https://aclanthology.org/2022.case-1.28},
  urldate = {2024-02-27},
  abstract = {The Event Causality Identification Shared Task of CASE 2022 involved two subtasks working on the Causal News Corpus. Subtask 1 required participants to predict if a sentence contains a causal relation or not. This is a supervised binary classification task. Subtask 2 required participants to identify the Cause, Effect and Signal spans per causal sentence. This could be seen as a supervised sequence labeling task. For both subtasks, participants uploaded their predictions for a held-out test set, and ranking was done based on binary F1 and macro F1 scores for Subtask 1 and 2, respectively. This paper summarizes the work of the 17 teams that submitted their results to our competition and 12 system description papers that were received. The best F1 scores achieved for Subtask 1 and 2 were 86.19\% and 54.15\%, respectively. All the top-performing approaches involved pretrained language models fine-tuned to the targeted task. We further discuss these approaches and analyze errors across participants’ systems in this paper.},
  eventtitle = {Proceedings of the 5th {{Workshop}} on {{Challenges}} and {{Applications}} of {{Automated Extraction}} of {{Socio-political Events}} from {{Text}} ({{CASE}})},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\WT7Y2P5W\Tan et al. - 2022 - Event Causality Identification with Causal News Co.pdf}
}

@online{tanUniCausalUnifiedBenchmark2023,
  title = {{{UniCausal}}: {{Unified Benchmark}} and {{Repository}} for {{Causal Text Mining}}},
  shorttitle = {{{UniCausal}}},
  author = {Tan, Fiona Anting and Zuo, Xinyu and Ng, See-Kiong},
  date = {2023-04-14},
  eprint = {2208.09163},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2208.09163},
  urldate = {2024-02-27},
  abstract = {Current causal text mining datasets vary in objectives, data coverage, and annotation schemes. These inconsistent efforts prevent modeling capabilities and fair comparisons of model performance. Furthermore, few datasets include cause-effect span annotations, which are needed for end-to-end causal relation extraction. To address these issues, we propose UniCausal, a unified benchmark for causal text mining across three tasks: (I) Causal Sequence Classification, (II) Cause-Effect Span Detection and (III) Causal Pair Classification. We consolidated and aligned annotations of six high quality, mainly human-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165 examples for each task respectively. Since the definition of causality can be subjective, our framework was designed to allow researchers to work on some or all datasets and tasks. To create an initial benchmark, we fine-tuned BERT pre-trained language models to each task, achieving 70.10\% Binary F1, 52.42\% Macro F1, and 84.68\% Binary F1 scores respectively.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  note = {Comment: 15 pages include References},
  file = {C:\Users\norouzin\Zotero\storage\UZVWH8II\Tan et al. - 2023 - UniCausal Unified Benchmark and Repository for Ca.pdf}
}

@article{thapaBeingHonestCausal2020,
  title = {Being Honest with Causal Language in Writing for Publication},
  author = {Thapa, Deependra K. and Visentin, Denis C. and Hunt, Glenn E. and Watson, Roger and Cleary, Michelle},
  date = {2020-06},
  journaltitle = {Journal of Advanced Nursing},
  shortjournal = {Journal of Advanced Nursing},
  volume = {76},
  number = {6},
  pages = {1285--1288},
  issn = {0309-2402, 1365-2648},
  doi = {10.1111/jan.14311},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/jan.14311},
  urldate = {2024-02-27},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\UAK3XTQN\Thapa et al. - 2020 - Being honest with causal language in writing for p.pdf}
}

@online{touvronLlamaOpenFoundation2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  date = {2023-07-19},
  eprint = {2307.09288},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.09288},
  urldate = {2024-02-27},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closedsource models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\norouzin\Zotero\storage\BSTCMAS4\Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf}
}

@article{vandevenNothingQuitePractical1989,
  title = {Nothing {{Is Quite So Practical}} as a {{Good Theory}}},
  author = {Van De Ven, Andrew H.},
  date = {1989-10},
  journaltitle = {Academy of Management Review},
  shortjournal = {AMR},
  volume = {14},
  number = {4},
  pages = {486--489},
  issn = {0363-7425, 1930-3807},
  doi = {10.5465/amr.1989.4308370},
  url = {http://journals.aom.org/doi/10.5465/amr.1989.4308370},
  urldate = {2024-02-27},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\AM5FLWNE\Van De Ven - 1989 - Nothing Is Quite So Practical as a Good Theory.pdf}
}

@article{vanlissaMappingPhenomenaRelevant2022,
  title = {Mapping {{Phenomena Relevant}} to {{Adolescent Emotion Regulation}}: {{A Text-Mining Systematic Review}}},
  shorttitle = {Mapping {{Phenomena Relevant}} to {{Adolescent Emotion Regulation}}},
  author = {Van Lissa, Caspar J.},
  date = {2022-03},
  journaltitle = {Adolescent Research Review},
  shortjournal = {Adolescent Res Rev},
  volume = {7},
  number = {1},
  pages = {127--139},
  issn = {2363-8346, 2363-8354},
  doi = {10.1007/s40894-021-00160-7},
  url = {https://link.springer.com/10.1007/s40894-021-00160-7},
  urldate = {2024-02-27},
  abstract = {Adolescence is a developmentally sensitive period for emotion regulation with potentially lifelong implications for mental health and well-being. Although substantial empirical research has addressed this topic, the literature is fragmented across subdisciplines, and an overarching theoretical framework is lacking. The first step toward constructing a unifying framework is identifying relevant phenomena. This systematic review of 6305 articles used text mining to identify phenomena relevant to adolescents’ emotion regulation. First, a baseline was established of relevant phenomena discussed in theory and recent narrative reviews. Then, article keywords and abstracts were analyzed using text mining, examining term frequency as an indicator of relevance and term co-occurrence as an indicator of association. The results reflected themes commonly featured in theory and narrative reviews, such as socialization and neurocognitive development, but also identified undertheorized themes, such as developmental disorders, physical health, external stressors, structural disadvantage, substance use, identity and moral development, and sexual development. The findings illustrate how text mining systematic reviews, a novel approach, may complement narrative reviews. Future theoretical work might integrate these undertheorized themes into an overarching framework, and empirical research might consider them as promising areas for future research, or as potential confounders in research on adolescents’ emotion regulation.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\GQ8S8HMJ\Van Lissa - 2022 - Mapping Phenomena Relevant to Adolescent Emotion R.pdf}
}

@unpublished{wolfHuggingfaceTransformersStateoftheart2019,
  title = {Huggingface's Transformers: {{State-of-the-art}} Natural Language Processing},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and others},
  date = {2019},
  eprint = {1910.03771},
  eprinttype = {arxiv}
}

@inproceedings{wolfTransformersStateoftheArtNatural2020,
  title = {Transformers: {{State-of-the-Art Natural Language Processing}}},
  shorttitle = {Transformers},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and Von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  date = {2020},
  pages = {38--45},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  urldate = {2024-02-27},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered stateof-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/ huggingface/transformers.},
  eventtitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{System Demonstrations}}},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\Y6JIWFHY\Wolf et al. - 2020 - Transformers State-of-the-Art Natural Language Pr.pdf}
}

@article{yangSurveyExtractionCausal2022,
  title = {A Survey on Extraction of Causal Relations from Natural Language Text},
  author = {Yang, Jie and Han, Soyeon Caren and Poon, Josiah},
  date = {2022-05},
  journaltitle = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {64},
  number = {5},
  pages = {1161--1186},
  issn = {0219-1377, 0219-3116},
  doi = {10.1007/s10115-022-01665-w},
  url = {https://link.springer.com/10.1007/s10115-022-01665-w},
  urldate = {2024-02-27},
  abstract = {As an essential component of human cognition, cause–effect relations appear frequently in text, and curating cause–effect relations from text helps in building causal networks for predictive tasks. Existing causality extraction techniques include knowledge-based, statistical machine learning (ML)-based, and deep learning-based approaches. Each method has its advantages and weaknesses. For example, knowledge-based methods are understandable but require extensive manual domain knowledge and have poor cross-domain applicability. Statistical machine learning methods are more automated because of natural language processing (NLP) toolkits. However, feature engineering is labor-intensive, and toolkits may lead to error propagation. In the past few years, deep learning techniques attract substantial attention from NLP researchers because of its powerful representation learning ability and the rapid increase in computational resources. Their limitations include high computational costs and a lack of adequate annotated training data. In this paper, we conduct a comprehensive survey of causality extraction. We initially introduce primary forms existing in the causality extraction: explicit intra-sentential causality, implicit causality, and inter-sentential causality. Next, we list benchmark datasets and modeling assessment methods for causal relation extraction. Then, we present a structured overview of the three techniques with their representative systems. Lastly, we highlight existing open challenges with their potential directions.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\GGJCHJYT\Yang et al. - 2022 - A survey on extraction of causal relations from na.pdf}
}

@article{zhaoBiomedicalCrosssentenceRelation2021,
  title = {Biomedical Cross-Sentence Relation Extraction via Multihead Attention and Graph Convolutional Networks},
  author = {Zhao, Di and Wang, Jian and Lin, Hongfei and Wang, Xin and Yang, Zhihao and Zhang, Yijia},
  date = {2021-06},
  journaltitle = {Applied Soft Computing},
  shortjournal = {Applied Soft Computing},
  volume = {104},
  pages = {107230},
  issn = {15684946},
  doi = {10.1016/j.asoc.2021.107230},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1568494621001538},
  urldate = {2024-02-27},
  abstract = {Most biomedical information extraction efforts are focused on binary relations, there is a strong need to extract drug–gene–mutation n-ary relations among cross-sentences. In recent years, endto-end biomedical relation extraction with sequence-based or dependency-based method has gained increasing attention. However, handling global dependencies and structural information remains challenges for sequence-based and dependency-based models. Joint exploitation of sequence and graph information may improve biomedical cross-sentence relation extraction. In this paper, we present a hybrid model for extracting biomedical relation in a cross-sentence which aims to address these problems. Our models rely on the self-attention mechanism that directly draws the global dependency relation of the sentence. Furthermore, to preserve the dependency structural information between the words that contain the syntactic dependency relations, we employ graph convolutional networks that encode the dependency structural information to guide the multihead attention to learn the dependency relation. Through extensive experiments on benchmark datasets, we demonstrated the effectiveness of our method.},
  langid = {english},
  file = {C:\Users\norouzin\Zotero\storage\SWKBGNXR\Zhao et al. - 2021 - Biomedical cross-sentence relation extraction via .pdf}
}
