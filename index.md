# Capturing Causal Claims: A Fine-Tuned Text Mining Model for Extracting Causal Sentences from Social Science Papers
## Abstract:
**_keywords_**: causal text mining, social science, parameter efficient tuning, causal sentences dataset
## 1. Introduction:
Understanding causality is an integral part of social scientific research. Causality is implicitly assumed when the results of social science research are used to inform decisions in treatment, policy, parenting, or clinical practice [@busettiCausalityGoodPractice2023]. Causal relationships are also crucial from a theoretical point of view, as a hallmark of “strong theories” is that they provide causal explanations for phenomena of interest [@hedstromCausalMechanismsSocial2010]. There are growing concerns over a “theory crisis” in the social sciences because many prevailing psychological theories are vague and non-specific with regard to causality, which makes it difficult to formulate testable hypotheses and impedes cumulative knowledge acquisition [@berkmanUsefulGoodTheory; @sanbonmatsuImpactComplexityMethods2021] . Scholars have recognized the importance of developing good theories that go beyond mere descriptions of empirical patterns. Good theories elucidate the underlying causal mechanisms [@vandevenNothingQuitePractical1989; @reiter2017theory]. As @{{@pearl2009causal} notes, understanding causality enables social scientists, practitioners, consultants, and policy makers to draw meaningful insights from research literature to prescribe empirically supported solutions and interventions. Causal assumptions play a key role in substantiating hypotheses and facilitating nuanced interpretations of findings, thus enriching scholarly discourse [@meehlAppraisingAmendingTheories1990; @reiter2017theory]. Given the importance of causality, it is unfortunate that social scientists do not always explicitly discuss causality [@grossStructureCausalChains2018]. To take stock of the current state of causal thought within a specific body of literature, this paper introduces a text mining model that extracts causal claims from full text papers. By automating the effortful procedure of coding statements as causal versus non-causal, this method enables comprehensive systematic reviews of causal claims in (large) bodies of literature. Our work contributes by curating a tailored dataset for social science literature and fine-tuning a model to extract causal sentences. This approach effectively addresses the challenges posed by the often ambiguous language used in social science literature to express causality, thereby substantially improving the accuracy and reliability of causal claim extraction from scholarly documents.
### 1.1 Why Care About Causality?
  Given the importance of causal assumptions for theory and practice, investigating existing causal claims in the published social scientific literature is a promising new area of meta-scientific research. An overview of existing causal claims can inform theory development, justify practical applications, and strengthen the methodological foundations that guide future theory-testing research. Due to the emergence of advanced text mining methods there has been a recent surge of interest in qualitative (or quantitatively aided qualitative) research synthesis methods. For instance, the use of text mining methods can be used to inductively identify important themes and relationships within a corpus of literature [@vanlissaMappingPhenomenaRelevant2022]. At present, there are some methods available to automatically extract causal claims from scientific text. A major shortcoming of existing techniques is that none have been developed with attention to the idiosyncrasies of social scientific writing.
### 1.2 Prior Work in Causality Detection
 Existing techniques for causal claims extraction can be broadly categorized into three main groups: knowledge-based methods, statistical machine learning methods, and deep learning methods [@yangSurveyExtractionCausal2022]. Knowledge-based methods rely on manually constructed rules, patterns, and domain expertise [@riaz2013toward]. While they are suitable for extracting straightforward causal relationships within single sentences, they require substantial human effort and struggle with implicit or complex cross-sentence causality. To define rules and patterns to recognize the nuanced connections between sentences, these methods must be meticulously crafted, demanding significant time and expertise. As an example of cross-sentence causality, consider the scenario where two distinct causes of divorce are mentioned in separate sentences: "Unfaithfulness is often cited as a leading cause of divorce. Meanwhile, financial stress is identified as another significant contributor."[@yangSurveyExtractionCausal2022].
Statistical machine learning methods, leverage natural language processing (NLP) techniques and annotated corpora to extract features, which are then fed into machine learning algorithms. These methods offer more automation than knowledge-based approaches but sometimes  require labor-intensive feature-engineering. The third category, deep learning methods automatically learn condensed numerical representations of words, allowing for better representation learning and cross-domain portability [@yangSurveyExtractionCausal2022]. Recurrent neural networks (RNNs), convolutional neural networks (CNNs), and graph convolutional networks (GCNs) are among the deep learning models commonly employed for causal relation extraction [@zhaoBiomedicalCrosssentenceRelation2021]. In the progression of NLP techniques, the advent of BERT (Bidirectional Encoder Representations from Transformers) introduced a paradigm shift towards models equipped with transformer mechanisms. These models, by design, excel in deciphering the context of words within sentences, showcasing a marked improvement over prior methodologies [@kenton2019bert]. The advancement of transformer models not only highlights the deficiencies of earlier approaches in understanding language nuances; it has also shown superior performance in benchmark evaluations, surpassing previous knowledge-based and statistical methods in the precision and recall of extracted causal relationships [@tanUniCausalUnifiedBenchmark2023].
Transformer models are trained on a large corpus of text, and the resulting model is called the pre-trained model. These pre-trained models can then be fine-tuned on specific downstream tasks and datasets. This pre-training feature allows for fine-tuning on specialized tasks using considerably smaller datasets, highlighting their adaptability and effectiveness across a wide range of applications [@chen2021better]. However, a notable concern is the substantial computational resources required, which has been somewhat alleviated by emerging techniques such as Parameter-Efficient tuning. This technique optimizes a small portion of model parameters while keeping the rest fixed, significantly reducing computation and storage costs [@dingParameterefficientFinetuningLargescale2023]. This development offers a promising way to harness the computational power of these models more efficiently. Given their profound understanding of context and nuanced capabilities, this study aims to employ transformer’s models to leverage their strengths in addressing challenges posed by limited data resources.
### 1.3 Causality Detection in Social Science
The extraction of causal relations from unstructured text poses significant challenges due to the complexity and variability of human language, as well as the diverse patterns used to represent causality. The problem is compounded because causal sentence extraction techniques developed in one scientific field often perform worse when applied in other fields;  a problem known as domain shift bias [@moghimifar2020domain]. Within the scope of social sciences, the challenges are intensified due to a pervasive reluctance to employ explicit causal language, a trend not commonly observed in other disciplines. This hesitation leads to vague and indirect expressions about causality, complicating the task of identifying causal connections. For instance, terms like "explain", "influence", and "predict", hint at causality but remain ambiguous about the actual nature of the relationships involved. This vagueness is a significant hurdle for causal sentence detection. This ambiguity is further compounded by the prevalent use of hedging and “weasel words” in scientific writing, which introduce uncertainty and obscure facts, often leading to misinterpretation or misrepresentation of the causal links being studied. This practice not only impedes clear understanding but also hampers the advancement of knowledge in these fields, as it fosters a culture of truthiness, where personal beliefs or assumptions are presented as facts without adequate evidence [@ottHedgingWeaselWords2018; @grosz2020taboo].
### 1.4 The Present Study
To address the challenges of ambiguity and lack of precision when extracting causal claims from social science text, this study set out to develop a fine-tuned model specifically designed for classifying causal and non-causal sentences in social science literature. Several candidate models were considered, including existing solutions and newly developed models based on prior research. This study further sets out to curate a dataset tailored specifically to the domain of social science, comprising 510 causal and 510 non-causal sentences extracted from the Cooperation Databank (CoDa); a comprehensive annotated archive of all studies on human cooperation(Spadaro, Tiddi et al. 2022). This dataset was also used to benchmark the performance of the candidate models. This paper contributes a manually curated dataset and fine-tuned causality detection model, which have the potential to significantly enhance the detection and analysis of causal claims in social scientific publications. Furthermore, the annotated data and tuned model serve as a foundation for further advancements in the extraction and analysis of causal sentences from the vast body of social science literature.
Following the fine-tuning of our model, we will address the following research questions:
- a) Which model works better in extracting causal language sentences from social science context? 
- b) Does fine-tuning models using domain specific training data confer a performance advantage?
- c) Does the performance of general causal sentence extraction models generalize to data from the context of social scientific literature?
- d) Does the curated dataset for social science contribute to increase the performance of general and domain specific models?
## 2. Method
In this section we are going to explain the steps that are done for training our model. As we are going to curate our domain-specific causal and non-causal claims dataset, firstly we will define the causal language that we used for the data curation process.
### 2.1 Definition of Causal Language
Causal language involves clauses or phrases where one event, state, action, or entity is explicitly portrayed as influencing another. Linguistic cues of causation abound, with causative verbs (such as 'increase', 'decrease', or 'improve') often denoting a strong causal relationship. Conjunctions such as 'because', 'due to', and 'since' are commonly used to express causality. It is noteworthy that academic writers, in an endeavor to make their research both accessible and compelling, might occasionally employ language that overstates causality (Thapa, Visentin et al. 2020).
### 2.2 Datasets
#### 2.2.1 General Purpose Dataset
We drew upon earlier work that  introduced six datasets for analyzing causal sentences (Tan, Zuo et al. , 2022). However, as the Penn Discourse Treebank (PDTB) dataset is not openly available, it was excluded from our study. The following is a summary of the datasets that were utilized to construct the combined corpus:
 **AltLex:** Annotated causal language utilizing alternative lexicalizations within single sentences from news articles. However, it has limitations including a small size, exclusion of implicit signals, and consideration solely of intra-sentence relations.
**BECAUSE 2.0:** Annotations of cause, effect, and connective spans based on Construction Grammar principles within single sentences from diverse sources. Its limitations comprise a modest size and omission of inter-sentence relations.
**CausalTimeBank (CTB):** Explicit causal relation annotations between events within the TempEval-3 corpus. It focuses solely on annotating events and disregards contextual information.
**EventStoryLine (ESL):** Annotations of both explicit and implicit causal relations between events in the Event Coreference Bank. It shares similar limitations with CTB in terms of event-centric annotation.
**SemEval 2010 Task 8:** Originally annotated for classifying semantic relations between noun phrases, with limitations including absence of contextual argument information and annotation restricted to inter-sentence relations.
Table 1 displays the number of causal and non-causal samples for both training and validation sets of each dataset separately. We should point out that the number of samples and its balance report is before preprocessing, as there were many duplicated samples in them.
**Table 1-** Five open-source datasets were utilized in this research
<table style="border-collapse:collapse;border-spacing:0" class="tg"><thead><tr><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Corpus</th><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Split</th><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Causal</th><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Non-Causal</th><th style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;font-weight:normal;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal">Sample Causal Sentence</th></tr></thead><tbody><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" rowspan="2"><br>AltLex</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">training</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">326</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">285</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal" rowspan="2">In a panic, he removed his mask,<br>inhaling a large amount of<br>phosgene gas which <span style="font-weight:bold">resulted in</span> <br>his death 72 hours later.</td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">test</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">130</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">286</td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" rowspan="2"><br><br>BECAUSE 2.0</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">training</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">473</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">107</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal" rowspan="2"><span style="font-weight:bold">Thanks to</span> the fast action of<br>the Federal Reserve in <br>cooperation with the SEC<br>and the Treasury, we dodged a <br>bullet when Bear <br>Stearns collapsed.</td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">test</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">15</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">4</td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" rowspan="2"><br>CausalTimeBank(CTB)</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">training</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">718</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">2599</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal" rowspan="2">In space, some say female pilots <br>were held up until now <span style="font-weight:bold">by </span>the lack <br>of piloting opportunities for them <br>in the military.</td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">test</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">100</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">392</td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:center;vertical-align:top;word-break:normal" rowspan="2"><br>EventStoryLine (ESL)</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">training</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">9146</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">8268</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal" rowspan="2"><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">3 dead </span><span style="font-weight:700;font-style:normal;text-decoration:none;color:#000;background-color:transparent">after</span><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent"> protesters torch Greek </span><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">bank Three people died in a burning </span><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">bank as tens of thousands of protesters </span><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">took to the streets of Athens during a </span><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">general strike over the Greek </span><br><span style="font-weight:400;font-style:normal;text-decoration:none;color:#000;background-color:transparent">government's planned spending cuts.</span></td></tr><tr><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">test</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">894</td><td style="border-color:inherit;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">1073</td></tr><tr><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal" rowspan="2"><br>SemEval 2010 Task 8</td><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">training</td><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">1003</td><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">6997</td><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal" rowspan="2">The current view is that the chronic <br>inflammation in the distal part of the <br>stomach caused by Helicobacter <br>pylori infection <span style="font-weight:bold">results in</span> an increased <br>acid production from the non-infected <br>upper corpus region of the stomach.</td></tr><tr><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">test</td><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">328</td><td style="border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;text-align:left;vertical-align:top;word-break:normal">2389</td></tr></tbody></table>

We combined the training and test sets of these datasets to make a general purpose dataset. Initially, it contained a significant amount of duplicate data, with 29,922 training and 5,611 test samples. After removing these duplicates, the dataset was reduced to 12,834 training and 3,679 test samples.The dataset was highly imbalanced (9,954 were causal and 2,880 were non-causal), posing a risk of bias during model fine-tuning. To counter this, we applied an undersampling technique resulting in a balanced general purpose dataset with 5,760 samples, equally divided between causal and non-causal sentences. Although the test set was imbalanced, with 3,070 non-causal and 609 causal sentences, we chose to leave it as is. This decision was made to ensure that the test set accurately reflects real-world scenarios, providing a true measure of the model's performance. The training set was then randomly split, assigning 20 percent for validation and the remainder for training.
#### 2.2.2 Curated Social Science Dataset
We manually curated an additional dataset aimed at understanding the use of causal language in social science literature based on The Cooperation Databank, a collection of all papers dedicated to game theory applications within social science (Spadaro, Tiddi et al. 2022). From this dataset, 2,590 articles were converted into raw text using the Grobid library in Python and subsequently segmented at the sentence level (Lopez 2009).
Following conversion, a post-processing stage corrected common errors arising during PDF-to-text translation. These errors typically involve misinterpretations of similar characters, such as "0" and "O," "b" and "6," or incorrect joining or splitting of letters.
The lead author labeled sentences using the Doccano web annotation tool (Nakayama et al., 2018). Sentences were cataloged as either causal, non-causal, or ambiguous. Instances marked as ambiguous  (117 of 1010 sentences; 11.58%) were subsequently reviewed by all authors. Inter-rater agreement was estimated using Fleiss' Kappa index (Fleiss 1971), resulting in $Kappa = 0.76$, denoting 'substantial' agreement. For those samples where consensus was elusive, a majority voting method was employed to finalize the labels. Ultimately, a number of 508 causal and 502 non-causal sentences were curated.
This dataset is balanced, and we  divided it into 70 percent for training, 10 percent for validation, and 20 percent for testing.
#### 2.2.3 Merged Dataset
This dataset was created by merging the social science training set and the general purpose training set, after removing duplicate samples and performing undersampling. Following the merge, 80% of the data was allocated for training, with the remaining 20% used for validation. 
